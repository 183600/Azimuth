// Azimuth Telemetry System - ä¸“é¡¹æµ‹è¯•ç”¨ä¾‹
// æ¶µç›–æ—¶é—´çª—å£èšåˆã€å¼‚å¸¸å¤„ç†ã€é…ç½®æ›´æ–°ç­‰é«˜çº§æµ‹è¯•åœºæ™¯

// æµ‹è¯•1: æ—¶é—´çª—å£èšåˆåŠŸèƒ½
test "æ—¶é—´çª—å£èšåˆè®¡ç®—æµ‹è¯•" {
  // åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®ç‚¹
  let time_series_data = [
    (1640995200000L, 10.5), // 2022-01-01 00:00:00
    (1640995260000L, 15.2), // 2022-01-01 00:01:00
    (1640995320000L, 12.8), // 2022-01-01 00:02:00
    (1640995380000L, 18.3), // 2022-01-01 00:03:00
    (1640995440000L, 14.7), // 2022-01-01 00:04:00
    (1640995500000L, 20.1), // 2022-01-01 00:05:00
    (1640995560000L, 16.9), // 2022-01-01 00:06:00
    (1640995620000L, 13.4), // 2022-01-01 00:07:00
    (1640995680000L, 19.6), // 2022-01-01 00:08:00
    (1640995740000L, 17.2)  // 2022-01-01 00:09:00
  ]
  
  // å®šä¹‰5åˆ†é’Ÿæ—¶é—´çª—å£
  let window_start = 1640995200000L
  let window_end = 1640995500000L
  
  // è¿‡æ»¤æ—¶é—´çª—å£å†…çš„æ•°æ®ç‚¹
  let window_data = time_series_data.filter(|(timestamp, _)| {
    timestamp >= window_start && timestamp <= window_end
  })
  
  // éªŒè¯è¿‡æ»¤ç»“æœ
  assert_eq(window_data.length(), 6)
  
  // è®¡ç®—èšåˆæŒ‡æ ‡
  let sum = window_data.reduce(|acc, (_, value)| acc + value, 0.0)
  let count = window_data.length().to_float()
  let avg = sum / count
  let max = window_data.reduce(|acc, (_, value)| if value > acc { value } else { acc }, 0.0)
  let min = window_data.reduce(|acc, (_, value)| if value < acc { value } else { acc }, 100.0)
  
  // éªŒè¯èšåˆç»“æœ
  assert_eq(sum, 91.6)
  assert_eq(count, 6.0)
  assert_eq(avg, 15.266666666666667)
  assert_eq(max, 20.1)
  assert_eq(min, 10.5)
  
  // æµ‹è¯•æ»‘åŠ¨çª—å£è®¡ç®—
  let sliding_window_size = 3
  let sliding_averages = []
  
  for i = 0; i <= window_data.length() - sliding_window_size; i = i + 1 {
    let window_sum = 0.0
    for j = i; j < i + sliding_window_size; j = j + 1 {
      window_sum = window_sum + window_data[j].1
    }
    let window_avg = window_sum / sliding_window_size.to_float()
    sliding_averages.push(window_avg)
  }
  
  // éªŒè¯æ»‘åŠ¨çª—å£ç»“æœ
  assert_eq(sliding_averages.length(), 4)
  assert_eq(sliding_averages[0], 12.833333333333334) // å‰3ä¸ªç‚¹å¹³å‡å€¼
  assert_eq(sliding_averages[3], 19.233333333333334) // å3ä¸ªç‚¹å¹³å‡å€¼
}

// æµ‹è¯•2: å¼‚å¸¸å¤„ç†å’Œæ¢å¤æœºåˆ¶
test "å¼‚å¸¸å¤„ç†å’Œæ¢å¤æœºåˆ¶æµ‹è¯•" {
  // æ¨¡æ‹Ÿå„ç§å¼‚å¸¸åœºæ™¯
  
  // åœºæ™¯1: ç½‘ç»œè¶…æ—¶å¼‚å¸¸
  let timeout_error = {
    "error.type": "timeout",
    "error.code": "NETWORK_TIMEOUT",
    "error.message": "Request timed out after 30 seconds",
    "retry.count": "3",
    "error.timestamp": "1640995200000"
  }
  
  // éªŒè¯é”™è¯¯ä¿¡æ¯ç»“æ„
  assert_eq(timeout_error["error.type"], "timeout")
  assert_eq(timeout_error["error.code"], "NETWORK_TIMEOUT")
  assert_eq(timeout_error["retry.count"], "3")
  
  // åœºæ™¯2: æ•°æ®åº“è¿æ¥å¼‚å¸¸
  let db_error = {
    "error.type": "database",
    "error.code": "CONNECTION_FAILED",
    "error.message": "Unable to connect to database",
    "db.host": "primary.db.example.com",
    "db.port": "5432",
    "fallback.used": "true"
  }
  
  // éªŒè¯æ•°æ®åº“é”™è¯¯ä¿¡æ¯
  assert_eq(db_error["error.type"], "database")
  assert_eq(db_error["fallback.used"], "true")
  
  // åœºæ™¯3: å†…å­˜ä¸è¶³å¼‚å¸¸
  let memory_error = {
    "error.type": "resource",
    "error.code": "OUT_OF_MEMORY",
    "error.message": "Memory allocation failed",
    "memory.requested": "512MB",
    "memory.available": "128MB",
    "gc.triggered": "true"
  }
  
  // éªŒè¯å†…å­˜é”™è¯¯ä¿¡æ¯
  assert_eq(memory_error["error.type"], "resource")
  assert_eq(memory_error["memory.requested"], "512MB")
  assert_eq(memory_error["gc.triggered"], "true")
  
  // æµ‹è¯•æ¢å¤ç­–ç•¥
  let recovery_strategies = [
    ("timeout", "exponential_backoff"),
    ("database", "failover_to_replica"),
    ("resource", "garbage_collection_and_retry")
  ]
  
  // éªŒè¯æ¢å¤ç­–ç•¥æ˜ å°„
  for (error_type, strategy) in recovery_strategies {
    match error_type {
      "timeout" => assert_eq(strategy, "exponential_backoff")
      "database" => assert_eq(strategy, "failover_to_replica")
      "resource" => assert_eq(strategy, "garbage_collection_and_retry")
      _ => assert_true(false)
    }
  }
  
  // æµ‹è¯•å¼‚å¸¸æ¢å¤æ—¶é—´è®¡ç®—
  let recovery_times = [5000, 10000, 20000, 40000] // æ¯«ç§’
  let max_recovery_time = 60000 // 60ç§’æœ€å¤§æ¢å¤æ—¶é—´
  
  for time in recovery_times {
    assert_true(time <= max_recovery_time)
  }
  
  // éªŒè¯æ€»æ¢å¤æ—¶é—´ä¸è¶…è¿‡é™åˆ¶
  let total_recovery_time = recovery_times.reduce(|acc, time| acc + time, 0)
  assert_true(total_recovery_time <= max_recovery_time * 2) // å…è®¸ä¸€å®šå®¹é”™
}

// æµ‹è¯•3: é…ç½®åŠ¨æ€æ›´æ–°
test "é…ç½®åŠ¨æ€æ›´æ–°æµ‹è¯•" {
  // åˆå§‹é…ç½®
  let initial_config = {
    "sampling.probability": "0.1",
    "batch.size": "100",
    "export.interval": "5000",
    "resource.attributes": "service.name=azimuth,service.version=1.0.0",
    "log.level": "INFO"
  }
  
  // éªŒè¯åˆå§‹é…ç½®
  assert_eq(initial_config["sampling.probability"], "0.1")
  assert_eq(initial_config["batch.size"], "100")
  assert_eq(initial_config["export.interval"], "5000")
  
  // æ›´æ–°é…ç½®1: é‡‡æ ·ç‡è°ƒæ•´
  let updated_config1 = initial_config |> 
    map_values(|key, value| 
      if key == "sampling.probability" { "0.2" } else { value }
    )
  
  assert_eq(updated_config1["sampling.probability"], "0.2")
  assert_eq(updated_config1["batch.size"], "100") // å…¶ä»–é…ç½®ä¿æŒä¸å˜
  
  // æ›´æ–°é…ç½®2: æ‰¹å¤„ç†å¤§å°è°ƒæ•´
  let updated_config2 = updated_config1 |> 
    map_values(|key, value| 
      if key == "batch.size" { "200" } else { value }
    )
  
  assert_eq(updated_config2["sampling.probability"], "0.2")
  assert_eq(updated_config2["batch.size"], "200")
  
  // æ›´æ–°é…ç½®3: å¤šé¡¹é…ç½®åŒæ—¶æ›´æ–°
  let updated_config3 = updated_config2 |> 
    map_values(|key, value| 
      match key {
        "export.interval" => "10000"
        "log.level" => "DEBUG"
        _ => value
      }
    )
  
  assert_eq(updated_config3["export.interval"], "10000")
  assert_eq(updated_config3["log.level"], "DEBUG")
  assert_eq(updated_config3["sampling.probability"], "0.2") // ä¹‹å‰æ›´æ–°ä¿æŒ
  
  // æµ‹è¯•é…ç½®éªŒè¯
  let config_validation_rules = [
    ("sampling.probability", |value: String| value.to_float() >= 0.0 && value.to_float() <= 1.0),
    ("batch.size", |value: String| value.to_int() > 0 && value.to_int() <= 1000),
    ("export.interval", |value: String| value.to_int() >= 1000)
  ]
  
  // éªŒè¯å½“å‰é…ç½®
  for (key, validator) in config_validation_rules {
    let value = updated_config3[key]
    assert_true(validator(value))
  }
  
  // æµ‹è¯•æ— æ•ˆé…ç½®æ‹’ç»
  let invalid_config = updated_config3 |> 
    map_values(|key, value| 
      if key == "sampling.probability" { "1.5" } else { value } // æ— æ•ˆé‡‡æ ·ç‡
    )
  
  // éªŒè¯æ— æ•ˆé…ç½®è¢«æ‹’ç»
  let invalid_sampling = invalid_config["sampling.probability"]
  assert_false(invalid_sampling.to_float() >= 0.0 && invalid_sampling.to_float() <= 1.0)
}

// æµ‹è¯•4: æ•°æ®ä¸€è‡´æ€§éªŒè¯
test "åˆ†å¸ƒå¼æ•°æ®ä¸€è‡´æ€§éªŒè¯æµ‹è¯•" {
  // æ¨¡æ‹Ÿå¤šä¸ªèŠ‚ç‚¹çš„æ•°æ®çŠ¶æ€
  let node_states = [
    ("node-1", {
      "last.update": "1640995200000",
      "data.version": "123",
      "checksum": "abc123def456",
      "status": "healthy"
    }),
    ("node-2", {
      "last.update": "1640995200000",
      "data.version": "123",
      "checksum": "abc123def456",
      "status": "healthy"
    }),
    ("node-3", {
      "last.update": "1640995195000", // ç¨æ—§çš„æ—¶é—´æˆ³
      "data.version": "122", // æ—§ç‰ˆæœ¬
      "checksum": "def456abc123", // ä¸åŒæ ¡éªŒå’Œ
      "status": "syncing"
    })
  ]
  
  // éªŒè¯èŠ‚ç‚¹æ•°é‡
  assert_eq(node_states.length(), 3)
  
  // æ£€æŸ¥ä¸€è‡´æ€§çŠ¶æ€
  let healthy_nodes = node_states.filter(|(_, state)| state["status"] == "healthy")
  let syncing_nodes = node_states.filter(|(_, state)| state["status"] == "syncing")
  
  assert_eq(healthy_nodes.length(), 2)
  assert_eq(syncing_nodes.length(), 1)
  
  // éªŒè¯å¥åº·èŠ‚ç‚¹æ•°æ®ä¸€è‡´æ€§
  let reference_node = healthy_nodes[0]
  for (_, state) in healthy_nodes {
    assert_eq(state["data.version"], reference_node.1["data.version"])
    assert_eq(state["checksum"], reference_node.1["checksum"])
  }
  
  // æµ‹è¯•æ•°æ®ä¿®å¤è¿‡ç¨‹
  let repaired_node3 = {
    "last.update": "1640995200000",
    "data.version": "123", // ä¿®å¤ä¸ºæœ€æ–°ç‰ˆæœ¬
    "checksum": "abc123def456", // ä¿®å¤ä¸ºæ­£ç¡®æ ¡éªŒå’Œ
    "status": "healthy" // çŠ¶æ€æ›´æ–°ä¸ºå¥åº·
  }
  
  // éªŒè¯ä¿®å¤ç»“æœ
  assert_eq(repaired_node3["data.version"], "123")
  assert_eq(repaired_node3["status"], "healthy")
  
  // æµ‹è¯•åˆ†å¸ƒå¼äº‹åŠ¡ä¸€è‡´æ€§
  let transaction_steps = [
    ("step1", "completed"),
    ("step2", "completed"),
    ("step3", "completed"),
    ("step4", "completed")
  ]
  
  // éªŒè¯æ‰€æœ‰æ­¥éª¤éƒ½å·²å®Œæˆ
  let all_completed = transaction_steps.all(|(_, status)| status == "completed")
  assert_true(all_completed)
  
  // æµ‹è¯•å›æ»šåœºæ™¯
  let failed_transaction = [
    ("step1", "completed"),
    ("step2", "completed"),
    ("step3", "failed"),
    ("step4", "rolled_back")
  ]
  
  // éªŒè¯å›æ»šçŠ¶æ€
  let has_rollback = failed_transaction.any(|(_, status)| status == "rolled_back")
  assert_true(has_rollback)
}

// æµ‹è¯•5: èµ„æºé™åˆ¶å’Œè¾¹ç•Œæ¡ä»¶
test "èµ„æºé™åˆ¶å’Œè¾¹ç•Œæ¡ä»¶æµ‹è¯•" {
  // æµ‹è¯•å†…å­˜é™åˆ¶åœºæ™¯
  let memory_limits = {
    "max.heap.size": "1GB",
    "max.direct.memory": "512MB",
    "gc.threshold": "0.8",
    "memory.warning.threshold": "0.7"
  }
  
  // éªŒè¯å†…å­˜é™åˆ¶é…ç½®
  assert_eq(memory_limits["max.heap.size"], "1GB")
  assert_eq(memory_limits["gc.threshold"], "0.8")
  
  // æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨æƒ…å†µ
  let memory_usage_scenarios = [
    ("normal", "0.5"), // 50% å†…å­˜ä½¿ç”¨
    ("warning", "0.75"), // 75% å†…å­˜ä½¿ç”¨ï¼Œè¶…è¿‡è­¦å‘Šé˜ˆå€¼
    ("critical", "0.85"), // 85% å†…å­˜ä½¿ç”¨ï¼Œè¶…è¿‡GCé˜ˆå€¼
    ("emergency", "0.95") // 95% å†…å­˜ä½¿ç”¨ï¼Œæ¥è¿‘æé™
  ]
  
  // éªŒè¯ä¸åŒä½¿ç”¨åœºæ™¯çš„å¤„ç†
  for (scenario, usage) in memory_usage_scenarios {
    let usage_float = usage.to_float()
    let warning_threshold = memory_limits["memory.warning.threshold"].to_float()
    let gc_threshold = memory_limits["gc.threshold"].to_float()
    
    match scenario {
      "normal" => assert_true(usage_float < warning_threshold)
      "warning" => assert_true(usage_float >= warning_threshold && usage_float < gc_threshold)
      "critical" => assert_true(usage_float >= gc_threshold)
      "emergency" => assert_true(usage_float > gc_threshold)
      _ => assert_true(false)
    }
  }
  
  // æµ‹è¯•è¿æ¥æ± é™åˆ¶
  let connection_pool_config = {
    "max.connections": "100",
    "min.connections": "10",
    "connection.timeout": "30000",
    "idle.timeout": "600000"
  }
  
  let max_connections = connection_pool_config["max.connections"].to_int()
  let min_connections = connection_pool_config["min.connections"].to_int()
  
  assert_true(max_connections > min_connections)
  
  // æ¨¡æ‹Ÿè¿æ¥æ± ä½¿ç”¨æƒ…å†µ
  let active_connections = 95
  assert_true(active_connections <= max_connections)
  
  // æµ‹è¯•çº¿ç¨‹æ± é™åˆ¶
  let thread_pool_config = {
    "core.threads": "5",
    "max.threads": "20",
    "queue.capacity": "1000",
    "thread.timeout": "60000"
  }
  
  let core_threads = thread_pool_config["core.threads"].to_int()
  let max_threads = thread_pool_config["max.threads"].to_int()
  
  assert_true(max_threads >= core_threads)
  
  // æµ‹è¯•é˜Ÿåˆ—æ»¡çš„æƒ…å†µ
  let queue_size = 1000
  let queue_capacity = thread_pool_config["queue.capacity"].to_int()
  assert_eq(queue_size, queue_capacity)
  
  // æµ‹è¯•è¾¹ç•Œæ¡ä»¶
  let boundary_values = [0, 1, -1, 2147483647, -2147483648]
  for value in boundary_values {
    // éªŒè¯ç³»ç»Ÿèƒ½å¤„ç†æå€¼
    let is_valid = value >= 0 && value <= 2147483647
    if value >= 0 {
      assert_true(is_valid)
    }
  }
  
  // æµ‹è¯•é›¶é™¤ä¿æŠ¤
  let numerator = 100
  let denominators = [1, 2, 4, 5, 10, 0]
  
  for denom in denominators {
    if denom != 0 {
      let result = numerator / denom
      assert_true(result >= 0)
    } else {
      // é›¶é™¤æƒ…å†µåº”è¯¥è¢«å®‰å…¨å¤„ç†
      assert_true(true)
    }
  }
}

// æµ‹è¯•6: å¤šæ ¼å¼æ•°æ®å¯¼å‡º
test "å¤šæ ¼å¼æ•°æ®å¯¼å‡ºæµ‹è¯•" {
  // å‡†å¤‡æµ‹è¯•æ•°æ®
  let telemetry_data = {
    "trace.id": "0af7651916cd43dd8448eb211c80319c",
    "span.id": "b7ad6b7169203331",
    "span.name": "http.request",
    "span.kind": "client",
    "start.time": "1640995200000",
    "end.time": "1640995201000",
    "duration": "1000",
    "status.code": "200",
    "attributes": "http.method=GET,http.url=/api/users,http.status_code=200"
  }
  
  // æµ‹è¯•JSONæ ¼å¼å¯¼å‡º
  let json_format = "{\n"
  json_format = json_format + "  \"trace.id\": \"" + telemetry_data["trace.id"] + "\",\n"
  json_format = json_format + "  \"span.id\": \"" + telemetry_data["span.id"] + "\",\n"
  json_format = json_format + "  \"span.name\": \"" + telemetry_data["span.name"] + "\",\n"
  json_format = json_format + "  \"duration\": " + telemetry_data["duration"] + "\n"
  json_format = json_format + "}"
  
  // éªŒè¯JSONæ ¼å¼
  assert_true(json_format.contains("\"trace.id\": \"0af7651916cd43dd8448eb211c80319c\""))
  assert_true(json_format.contains("\"duration\": 1000"))
  
  // æµ‹è¯•CSVæ ¼å¼å¯¼å‡º
  let csv_headers = "trace.id,span.id,span.name,duration,status.code"
  let csv_values = telemetry_data["trace.id"] + "," + 
                  telemetry_data["span.id"] + "," + 
                  telemetry_data["span.name"] + "," + 
                  telemetry_data["duration"] + "," + 
                  telemetry_data["status.code"]
  
  let csv_format = csv_headers + "\n" + csv_values
  
  // éªŒè¯CSVæ ¼å¼
  assert_true(csv_format.contains("trace.id,span.id,span.name"))
  assert_true(csv_format.contains("0af7651916cd43dd8448eb211c80319c,b7ad6b7169203331"))
  
  // æµ‹è¯•XMLæ ¼å¼å¯¼å‡º
  let xml_format = "<telemetry>\n"
  xml_format = xml_format + "  <trace id=\"" + telemetry_data["trace.id"] + "\">\n"
  xml_format = xml_format + "    <span id=\"" + telemetry_data["span.id"] + "\" name=\"" + telemetry_data["span.name"] + "\">\n"
  xml_format = xml_format + "      <duration>" + telemetry_data["duration"] + "</duration>\n"
  xml_format = xml_format + "      <status code=\"" + telemetry_data["status.code"] + "\" />\n"
  xml_format = xml_format + "    </span>\n"
  xml_format = xml_format + "  </trace>\n"
  xml_format = xml_format + "</telemetry>"
  
  // éªŒè¯XMLæ ¼å¼
  assert_true(xml_format.contains("<telemetry>"))
  assert_true(xml_format.contains("<span id=\"b7ad6b7169203331\""))
  assert_true(xml_format.contains("<duration>1000</duration>"))
  
  // æµ‹è¯•PrometheusæŒ‡æ ‡æ ¼å¼
  let prometheus_format = "# HELP http_request_duration_seconds Duration of HTTP requests\n"
  prometheus_format = prometheus_format + "# TYPE http_request_duration_seconds histogram\n"
  prometheus_format = prometheus_format + "http_request_duration_seconds_bucket{le=\"1.0\"} 1\n"
  prometheus_format = prometheus_format + "http_request_duration_seconds_bucket{le=\"+Inf\"} 1\n"
  prometheus_format = prometheus_format + "http_request_duration_seconds_sum 1.0\n"
  prometheus_format = prometheus_format + "http_request_duration_seconds_count 1\n"
  
  // éªŒè¯Prometheusæ ¼å¼
  assert_true(prometheus_format.contains("# HELP http_request_duration_seconds"))
  assert_true(prometheus_format.contains("# TYPE http_request_duration_seconds histogram"))
  assert_true(prometheus_format.contains("http_request_duration_seconds_sum 1.0"))
  
  // æµ‹è¯•OpenTelemetryåè®®æ ¼å¼
  let otlp_format = "{
  \"resourceSpans\": [{
    \"resource\": {
      \"attributes\": [{
        \"key\": \"service.name\",
        \"value\": {\"stringValue\": \"azimuth-service\"}
      }]
    },
    \"scopeSpans\": [{
      \"scope\": {
        \"name\": \"azimuth.telemetry\",
        \"version\": \"0.1.0\"
      },
      \"spans\": [{
        \"traceId\": \"" + telemetry_data["trace.id"] + "\",
        \"spanId\": \"" + telemetry_data["span.id"] + "\",
        \"name\": \"" + telemetry_data["span.name"] + "\",
        \"kind\": " + (if telemetry_data["span.kind"] == "client" { "3" } else { "1" }) + ",
        \"startTimeUnixNano\": \"" + telemetry_data["start.time"] + "000000\",
        \"endTimeUnixNano\": \"" + telemetry_data["end.time"] + "000000\",
        \"status\": {\"code\": \"OK\"}
      }]
    }]
  }]
}"
  
  // éªŒè¯OTLPæ ¼å¼
  assert_true(otlp_format.contains("\"resourceSpans\""))
  assert_true(otlp_format.contains("\"traceId\": \"0af7651916cd43dd8448eb211c80319c\""))
  assert_true(otlp_format.contains("\"name\": \"http.request\""))
}

// æµ‹è¯•7: ç¼“å­˜æ€§èƒ½å’Œç­–ç•¥
test "ç¼“å­˜æ€§èƒ½å’Œç­–ç•¥æµ‹è¯•" {
  // æµ‹è¯•LRUç¼“å­˜ç­–ç•¥
  let cache_capacity = 5
  let mut cache_entries = []
  
  // æ·»åŠ ç¼“å­˜é¡¹ï¼Œè¶…è¿‡å®¹é‡
  let keys = ["key1", "key2", "key3", "key4", "key5", "key6", "key7"]
  for key in keys {
    cache_entries.push((key, "value." + key))
    
    // å¦‚æœè¶…è¿‡å®¹é‡ï¼Œç§»é™¤æœ€æ—§çš„é¡¹
    if cache_entries.length() > cache_capacity {
      cache_entries = cache_entries.slice(1, cache_entries.length()) // ç§»é™¤ç¬¬ä¸€é¡¹
    }
  }
  
  // éªŒè¯ç¼“å­˜å®¹é‡é™åˆ¶
  assert_eq(cache_entries.length(), cache_capacity)
  
  // éªŒè¯ä¿ç•™çš„æ˜¯æœ€æ–°çš„é¡¹
  assert_eq(cache_entries[cache_entries.length() - 1].0, "key7") // æœ€æ–°é¡¹
  assert_eq(cache_entries[0].0, "key3") // æœ€æ—§çš„æ–°é¡¹
  
  // æµ‹è¯•ç¼“å­˜å‘½ä¸­ç‡
  let cache_operations = [
    ("key3", "hit"), // åœ¨ç¼“å­˜ä¸­
    ("key4", "hit"), // åœ¨ç¼“å­˜ä¸­
    ("key8", "miss"), // ä¸åœ¨ç¼“å­˜ä¸­
    ("key5", "hit"), // åœ¨ç¼“å­˜ä¸­
    ("key9", "miss"), // ä¸åœ¨ç¼“å­˜ä¸­
    ("key7", "hit"), // åœ¨ç¼“å­˜ä¸­
    ("key10", "miss") // ä¸åœ¨ç¼“å­˜ä¸­
  ]
  
  let hits = cache_operations.filter(|(_, result)| result == "hit").length()
  let misses = cache_operations.filter(|(_, result)| result == "miss").length()
  let total_operations = cache_operations.length()
  
  // éªŒè¯å‘½ä¸­ç‡è®¡ç®—
  assert_eq(hits, 4)
  assert_eq(misses, 3)
  assert_eq(total_operations, 7)
  
  let hit_rate = hits.to_float() / total_operations.to_float()
  assert_eq(hit_rate, 0.5714285714285714) // çº¦57.14%
  
  // æµ‹è¯•TTLï¼ˆç”Ÿå­˜æ—¶é—´ï¼‰ç­–ç•¥
  let cache_with_ttl = [
    ("user:123", "user_data", 1640995200000L), // æ—¶é—´æˆ³
    ("user:456", "user_data", 1640995260000L), // 1åˆ†é’Ÿå
    ("user:789", "user_data", 1640995320000L)  // 2åˆ†é’Ÿå
  ]
  
  let current_time = 1640995380000L // 3åˆ†é’Ÿå
  let ttl_seconds = 120 // 2åˆ†é’ŸTTL
  
  // è¿‡æ»¤è¿‡æœŸé¡¹
  let valid_entries = cache_with_ttl.filter(|(_, _, timestamp)| {
    (current_time - timestamp) / 1000 <= ttl_seconds
  })
  
  // éªŒè¯TTLè¿‡æ»¤ç»“æœ
  assert_eq(valid_entries.length(), 1) // åªæœ‰æœ€åä¸€é¡¹æœ‰æ•ˆ
  
  // æµ‹è¯•ç¼“å­˜é¢„çƒ­
  let preload_keys = ["config:app", "config:db", "config:cache"]
  let preloaded_cache = []
  
  for key in preload_keys {
    preloaded_cache.push((key, "preloaded.value." + key))
  }
  
  // éªŒè¯é¢„çƒ­ç¼“å­˜
  assert_eq(preloaded_cache.length(), 3)
  assert_true(preloaded_cache.any(|(k, _)| k == "config:app"))
  
  // æµ‹è¯•ç¼“å­˜åˆ·æ–°ç­–ç•¥
  let refresh_policies = [
    ("write_through", "immediate"),
    ("write_behind", "batched"),
    ("refresh_ahead", "proactive")
  ]
  
  for (policy, behavior) in refresh_policies {
    match policy {
      "write_through" => assert_eq(behavior, "immediate")
      "write_behind" => assert_eq(behavior, "batched")
      "refresh_ahead" => assert_eq(behavior, "proactive")
      _ => assert_true(false)
    }
  }
}

// æµ‹è¯•8: å®‰å…¨æ€§å’Œæƒé™æ§åˆ¶
test "å®‰å…¨æ€§å’Œæƒé™æ§åˆ¶æµ‹è¯•" {
  // æµ‹è¯•APIå¯†é’¥éªŒè¯
  let api_keys = [
    ("key_readonly", "ro_abc123def456", ["read"]),
    ("key_readwrite", "rw_def789ghi012", ["read", "write"]),
    ("key_admin", "admin_ghi345jkl678", ["read", "write", "admin"])
  ]
  
  // éªŒè¯APIå¯†é’¥æ ¼å¼
  for (name, key, permissions) in api_keys {
    match name {
      "key_readonly" => {
        assert_true(key.starts_with("ro_"))
        assert_eq(permissions.length(), 1)
        assert_true(permissions.contains("read"))
      }
      "key_readwrite" => {
        assert_true(key.starts_with("rw_"))
        assert_eq(permissions.length(), 2)
        assert_true(permissions.contains("read"))
        assert_true(permissions.contains("write"))
      }
      "key_admin" => {
        assert_true(key.starts_with("admin_"))
        assert_eq(permissions.length(), 3)
        assert_true(permissions.contains("admin"))
      }
      _ => assert_true(false)
    }
  }
  
  // æµ‹è¯•æƒé™æ£€æŸ¥
  let permission_checks = [
    ("ro_abc123def456", "read", true),
    ("ro_abc123def456", "write", false),
    ("ro_abc123def456", "admin", false),
    ("rw_def789ghi012", "read", true),
    ("rw_def789ghi012", "write", true),
    ("rw_def789ghi012", "admin", false),
    ("admin_ghi345jkl678", "read", true),
    ("admin_ghi345jkl678", "write", true),
    ("admin_ghi345jkl678", "admin", true)
  ]
  
  for (key, action, expected) in permission_checks {
    let key_info = api_keys.find(|(_, k, _)| k == key)
    match key_info {
      Some((_, _, permissions)) => {
        let has_permission = permissions.contains(action)
        assert_eq(has_permission, expected)
      }
      None => assert_true(false)
    }
  }
  
  // æµ‹è¯•æ•°æ®è„±æ•
  let sensitive_data = {
    "user.id": "12345",
    "user.email": "user@example.com",
    "user.phone": "+1234567890",
    "credit.card": "4111111111111111",
    "ssn": "123-45-6789"
  }
  
  // å®šä¹‰è„±æ•è§„åˆ™
  let masking_rules = [
    ("user.email", |value: String| {
      let parts = value.split("@")
      if parts.length() == 2 {
        let username = parts[0]
        let domain = parts[1]
        if username.length() > 2 {
          username.slice(0, 2) + "***@" + domain
        } else {
          "***@" + domain
        }
      } else {
        "***"
      }
    }),
    ("user.phone", |value: String| {
      if value.length() >= 4 {
        "***" + value.slice(value.length() - 4, value.length())
      } else {
        "***"
      }
    }),
    ("credit.card", |value: String| {
      if value.length() >= 4 {
        "****-****-****-" + value.slice(value.length() - 4, value.length())
      } else {
        "****"
      }
    }),
    ("ssn", |value: String| "***-**-****")
  ]
  
  // åº”ç”¨è„±æ•è§„åˆ™
  let masked_data = sensitive_data
  for (field, mask_func) in masking_rules {
    let original_value = sensitive_data[field]
    let masked_value = mask_func(original_value)
    // éªŒè¯è„±æ•æ•ˆæœ
    match field {
      "user.email" => assert_true(masked_value.contains("***"))
      "user.phone" => assert_true(masked_value.starts_with("***"))
      "credit.card" => assert_true(masked_value.starts_with("****"))
      "ssn" => assert_eq(masked_value, "***-**-****")
      _ => assert_true(false)
    }
  }
  
  // æµ‹è¯•è®¿é—®æ—¥å¿—è®°å½•
  let access_logs = []
  
  // æ¨¡æ‹Ÿè®¿é—®å°è¯•
  let access_attempts = [
    ("user1", "ro_abc123def456", "/api/metrics", "read", "success"),
    ("user2", "ro_abc123def456", "/api/config", "write", "denied"),
    ("user3", "rw_def789ghi012", "/api/traces", "write", "success"),
    ("user4", "invalid_key", "/api/logs", "read", "denied")
  ]
  
  // è®°å½•è®¿é—®æ—¥å¿—
  for (user, key, endpoint, action, result) in access_attempts {
    let log_entry = user + "," + key + "," + endpoint + "," + action + "," + result
    access_logs.push(log_entry)
  }
  
  // éªŒè¯è®¿é—®æ—¥å¿—
  assert_eq(access_logs.length(), 4)
  
  let denied_attempts = access_logs.filter(|log| log.contains("denied"))
  assert_eq(denied_attempts.length(), 2)
  
  let successful_attempts = access_logs.filter(|log| log.contains("success"))
  assert_eq(successful_attempts.length(), 2)
}

// æµ‹è¯•9: è·¨å¹³å°å…¼å®¹æ€§
test "è·¨å¹³å°å…¼å®¹æ€§æµ‹è¯•" {
  // æµ‹è¯•ä¸åŒæ“ä½œç³»ç»Ÿçš„è·¯å¾„å¤„ç†
  let platforms = [
    ("windows", "C:\\Program Files\\Azimuth\\logs\\app.log"),
    ("linux", "/var/log/azimuth/app.log"),
    ("macos", "/usr/local/var/log/azimuth/app.log"),
    ("docker", "/app/logs/azimuth.log")
  ]
  
  // éªŒè¯è·¯å¾„æ ¼å¼
  for (platform, path) in platforms {
    match platform {
      "windows" => assert_true(path.contains("\\"))
      "linux" => assert_true(path.starts_with("/"))
      "macos" => assert_true(path.starts_with("/usr/local"))
      "docker" => assert_true(path.starts_with("/app"))
      _ => assert_true(false)
    }
  }
  
  // æµ‹è¯•ç¯å¢ƒå˜é‡å¤„ç†
  let env_configs = [
    ("AZIMUTH_HOME", "C:\\Azimuth", "windows"),
    ("AZIMUTH_HOME", "/opt/azimuth", "linux"),
    ("AZIMUTH_CONFIG", "/etc/azimuth/config.yaml", "linux"),
    ("AZIMUTH_CONFIG", "C:\\ProgramData\\Azimuth\\config.yaml", "windows")
  ]
  
  // éªŒè¯ç¯å¢ƒå˜é‡é…ç½®
  for (var_name, var_value, platform) in env_configs {
    match platform {
      "windows" => assert_true(var_value.contains(":\\"))
      "linux" => assert_true(var_value.starts_with("/"))
      _ => assert_true(false)
    }
  }
  
  // æµ‹è¯•ä¸åŒæ¶æ„çš„å…¼å®¹æ€§
  let architectures = ["x86_64", "arm64", "amd64", "arm32"]
  
  for arch in architectures {
    // éªŒè¯æ¶æ„æ ‡è¯†ç¬¦
    match arch {
      "x86_64" => assert_true(arch.contains("64"))
      "arm64" => assert_true(arch.contains("64"))
      "amd64" => assert_true(arch.contains("64"))
      "arm32" => assert_true(arch.contains("32"))
      _ => assert_true(false)
    }
  }
  
  // æµ‹è¯•æ—¶åŒºå¤„ç†
  let timezones = [
    ("UTC", "2022-01-01T00:00:00Z"),
    ("America/New_York", "2021-12-31T19:00:00-05:00"),
    ("Europe/London", "2022-01-01T00:00:00+00:00"),
    ("Asia/Tokyo", "2022-01-01T09:00:00+09:00"),
    ("Australia/Sydney", "2022-01-01T11:00:00+11:00")
  ]
  
  // éªŒè¯æ—¶åŒºæ ¼å¼
  for (tz_name, tz_time) in timezones {
    match tz_name {
      "UTC" => assert_true(tz_time.ends_with("Z"))
      "America/New_York" => assert_true(tz_time.contains("-05:00"))
      "Europe/London" => assert_true(tz_time.contains("+00:00"))
      "Asia/Tokyo" => assert_true(tz_time.contains("+09:00"))
      "Australia/Sydney" => assert_true(tz_time.contains("+11:00"))
      _ => assert_true(false)
    }
  }
  
  // æµ‹è¯•å­—ç¬¦ç¼–ç å¤„ç†
  let encodings = [
    ("UTF-8", "æµ‹è¯•ä¸­æ–‡"),
    ("UTF-16", "ğŸŒŸ Unicode Emoji"),
    ("ASCII", "Plain ASCII text"),
    ("ISO-8859-1", "Text with Ã©Ã Ã§")
  ]
  
  // éªŒè¯ç¼–ç å¤„ç†
  for (encoding, text) in encodings {
    match encoding {
      "UTF-8" => assert_true(text.length() > 0)
      "UTF-16" => assert_true(text.contains("ğŸŒŸ"))
      "ASCII" => assert_true(text == "Plain ASCII text")
      "ISO-8859-1" => assert_true(text.contains("Ã©"))
      _ => assert_true(false)
    }
  }
  
  // æµ‹è¯•ç½‘ç»œåè®®å…¼å®¹æ€§
  let protocols = [
    ("HTTP/1.1", "GET /api/metrics HTTP/1.1"),
    ("HTTP/2", "PRI * HTTP/2.0"),
    ("gRPC", "Content-Type: application/grpc"),
    ("WebSocket", "Upgrade: websocket")
  ]
  
  // éªŒè¯åè®®æ ‡è¯†
  for (protocol, signature) in protocols {
    match protocol {
      "HTTP/1.1" => assert_true(signature.contains("HTTP/1.1"))
      "HTTP/2" => assert_true(signature.contains("HTTP/2.0"))
      "gRPC" => assert_true(signature.contains("application/grpc"))
      "WebSocket" => assert_true(signature.contains("websocket"))
      _ => assert_true(false)
    }
  }
}

// æµ‹è¯•10: æ€§èƒ½åŸºå‡†æµ‹è¯•
test "æ€§èƒ½åŸºå‡†æµ‹è¯•" {
  // æµ‹è¯•Spanåˆ›å»ºæ€§èƒ½
  let span_creation_start = 1640995200000L // æ¨¡æ‹Ÿé«˜ç²¾åº¦æ—¶é—´æˆ³
  let span_count = 1000
  
  // æ¨¡æ‹Ÿæ‰¹é‡åˆ›å»ºSpan
  let spans = []
  for i = 0; i < span_count; i = i + 1 {
    let span_name = "benchmark.span." + i.to_string()
    spans.push(span_name)
  }
  
  let span_creation_end = 1640995200500L // æ¨¡æ‹Ÿåˆ›å»ºå®Œæˆæ—¶é—´
  let span_creation_duration = span_creation_end - span_creation_start
  
  // éªŒè¯åˆ›å»ºæ€§èƒ½
  assert_eq(spans.length(), span_count)
  assert_eq(span_creation_duration, 500L) // 500æ¯«ç§’
  
  let spans_per_second = (span_count.to_float() / span_creation_duration.to_float()) * 1000.0
  assert_eq(spans_per_second, 2000.0) // æ¯ç§’2000ä¸ªSpan
  
  // æµ‹è¯•Metricè®°å½•æ€§èƒ½
  let metric_recording_start = 1640995201000L
  let metric_count = 5000
  
  // æ¨¡æ‹Ÿæ‰¹é‡è®°å½•æŒ‡æ ‡
  let metrics = []
  for i = 0; i < metric_count; i = i + 1 {
    let metric_value = i.to_float() * 1.5
    metrics.push(metric_value)
  }
  
  let metric_recording_end = 1640995202000L
  let metric_recording_duration = metric_recording_end - metric_recording_start
  
  // éªŒè¯æŒ‡æ ‡è®°å½•æ€§èƒ½
  assert_eq(metrics.length(), metric_count)
  assert_eq(metric_recording_duration, 1000L) // 1ç§’
  
  let metrics_per_second = metric_count.to_float() / (metric_recording_duration.to_float() / 1000.0)
  assert_eq(metrics_per_second, 5000.0) // æ¯ç§’5000ä¸ªæŒ‡æ ‡
  
  // æµ‹è¯•Logè®°å½•æ€§èƒ½
  let log_recording_start = 1640995203000L
  let log_count = 2000
  
  // æ¨¡æ‹Ÿæ‰¹é‡è®°å½•æ—¥å¿—
  let logs = []
  for i = 0; i < log_count; i = i + 1 {
    let log_message = "Benchmark log message " + i.to_string()
    logs.push(log_message)
  }
  
  let log_recording_end = 1640995203500L
  let log_recording_duration = log_recording_end - log_recording_start
  
  // éªŒè¯æ—¥å¿—è®°å½•æ€§èƒ½
  assert_eq(logs.length(), log_count)
  assert_eq(log_recording_duration, 500L) // 500æ¯«ç§’
  
  let logs_per_second = log_count.to_float() / (log_recording_duration.to_float() / 1000.0)
  assert_eq(logs_per_second, 4000.0) // æ¯ç§’4000æ¡æ—¥å¿—
  
  // æµ‹è¯•åºåˆ—åŒ–æ€§èƒ½
  let serialization_start = 1640995204000L
  let data_size = 1000
  
  // æ¨¡æ‹Ÿæ‰¹é‡åºåˆ—åŒ–
  let serialized_data = []
  for i = 0; i < data_size; i = i + 1 {
    let data_point = "data.point." + i.to_string() + "=" + (i * 2).to_string()
    serialized_data.push(data_point)
  }
  
  // åˆå¹¶åºåˆ—åŒ–æ•°æ®
  let combined_data = serialized_data.reduce(|acc, data| acc + "," + data, "")
  
  let serialization_end = 1640995204200L
  let serialization_duration = serialization_end - serialization_start
  
  // éªŒè¯åºåˆ—åŒ–æ€§èƒ½
  assert_eq(serialized_data.length(), data_size)
  assert_true(combined_data.length() > 0)
  assert_eq(serialization_duration, 200L) // 200æ¯«ç§’
  
  // æµ‹è¯•å†…å­˜ä½¿ç”¨æ•ˆç‡
  let memory_usage_before = 100 * 1024 * 1024 // 100MB
  let memory_usage_after = 105 * 1024 * 1024 // 105MB
  
  let memory_increase = memory_usage_after - memory_usage_before
  let memory_increase_mb = memory_increase / (1024 * 1024)
  
  // éªŒè¯å†…å­˜ä½¿ç”¨æ•ˆç‡
  assert_eq(memory_increase_mb, 5) // å¢åŠ 5MB
  
  let memory_per_span = memory_increase.to_float() / span_count.to_float()
  assert_eq(memory_per_span, 5120.0) // æ¯ä¸ªSpançº¦5KB
  
  // æµ‹è¯•ååé‡åŸºå‡†
  let benchmark_results = [
    ("span.creation", 2000.0, "spans/second"),
    ("metric.recording", 5000.0, "metrics/second"),
    ("log.recording", 4000.0, "logs/second"),
    ("serialization", 5000.0, "items/second")
  ]
  
  // éªŒè¯ååé‡ç»“æœ
  for (operation, throughput, unit) in benchmark_results {
    match operation {
      "span.creation" => assert_eq(throughput, 2000.0)
      "metric.recording" => assert_eq(throughput, 5000.0)
      "log.recording" => assert_eq(throughput, 4000.0)
      "serialization" => assert_eq(throughput, 5000.0)
      _ => assert_true(false)
    }
  }
  
  // æµ‹è¯•å»¶è¿ŸåŸºå‡†
  let latency_benchmarks = [
    ("span.create", 0.5, "ms"),
    ("metric.record", 0.2, "ms"),
    ("log.emit", 0.25, "ms"),
    ("context.inject", 0.1, "ms"),
    ("context.extract", 0.15, "ms")
  ]
  
  // éªŒè¯å»¶è¿ŸåŸºå‡†
  for (operation, latency, unit) in latency_benchmarks {
    match operation {
      "span.create" => assert_eq(latency, 0.5)
      "metric.record" => assert_eq(latency, 0.2)
      "log.emit" => assert_eq(latency, 0.25)
      "context.inject" => assert_eq(latency, 0.1)
      "context.extract" => assert_eq(latency, 0.15)
      _ => assert_true(false)
    }
  }
}