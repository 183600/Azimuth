// åºåˆ—åŒ–å’Œååºåˆ—åŒ–æµ‹è¯•ç”¨ä¾‹
// æµ‹è¯•é¥æµ‹æ•°æ®çš„åºåˆ—åŒ–æ ¼å¼å…¼å®¹æ€§å’Œè½¬æ¢

test "json_serialization_compatibility" {
  // æµ‹è¯•JSONåºåˆ—åŒ–å…¼å®¹æ€§
  
  let telemetry_data = {
    "trace_id": "0af7651916cd43dd8448eb211c80319c",
    "span_id": "b7ad6b7169203331",
    "parent_span_id": "a1b2c3d4e5f67890",
    "operation_name": "http_request",
    "start_time": "2023-01-01T00:00:00Z",
    "duration_ms": 250,
    "status": "OK",
    "attributes": [
      {"key": "http.method", "value": {"type": "string", "value": "GET"}},
      {"key": "http.status_code", "value": {"type": "int", "value": 200}},
      {"key": "http.url", "value": {"type": "string", "value": "/api/users"}}
    ]
  }
  
  // æ¨¡æ‹ŸJSONåºåˆ—åŒ–
  let json_string = "{"
    + "\"trace_id\":\"" + telemetry_data["trace_id"] + "\","
    + "\"span_id\":\"" + telemetry_data["span_id"] + "\","
    + "\"operation_name\":\"" + telemetry_data["operation_name"] + "\","
    + "\"duration_ms\":" + telemetry_data["duration_ms"].to_string() + ","
    + "\"status\":\"" + telemetry_data["status"] + "\""
    + "}"
  
  // éªŒè¯JSONåºåˆ—åŒ–
  assert_eq(json_string.has_prefix("{"), true)
  assert_eq(json_string.has_suffix("}"), true)
  assert_eq(json_string.contains("trace_id"), true)
  assert_eq(json_string.contains("span_id"), true)
  assert_eq(json_string.contains("duration_ms"), true)
  
  // æ¨¡æ‹ŸJSONååºåˆ—åŒ–éªŒè¯
  let json_contains_trace_id = json_string.contains("\"trace_id\":\"0af7651916cd43dd8448eb211c80319c\"")
  let json_contains_duration = json_string.contains("\"duration_ms\":250")
  
  assert_eq(json_contains_trace_id, true)
  assert_eq(json_contains_duration, true)
}

test "protobuf_serialization_format" {
  // æµ‹è¯•Protobufåºåˆ—åŒ–æ ¼å¼
  
  let metric_data = {
    "name": "http_requests_total",
    "unit": "count",
    "type": "counter",
    "value": 1000,
    "labels": [
      {"key": "method", "value": "GET"},
      {"key": "status", "value": "200"},
      {"key": "endpoint", "value": "/api/users"}
    ],
    "timestamp": "2023-01-01T00:00:00Z"
  }
  
  // æ¨¡æ‹ŸProtobufåºåˆ—åŒ–ï¼ˆç®€åŒ–ç‰ˆï¼‰
  let proto_fields = []
  proto_fields.push("field1:" + metric_data["name"])
  proto_fields.push("field2:" + metric_data["unit"])
  proto_fields.push("field3:" + metric_data["value"].to_string())
  
  let proto_string = proto_fields.join(",")
  
  // éªŒè¯Protobufåºåˆ—åŒ–
  assert_eq(proto_string.contains("field1:http_requests_total"), true)
  assert_eq(proto_string.contains("field2:count"), true)
  assert_eq(proto_string.contains("field3:1000"), true)
  
  // éªŒè¯å­—æ®µé¡ºåºå’Œæ•°é‡
  let field_count = proto_string.split(",").length()
  assert_eq(field_count, 3)
}

test "attribute_value_serialization" {
  // æµ‹è¯•å±æ€§å€¼åºåˆ—åŒ–
  
  let attributes = [
    ("string_attr", AttributeValue::string("test_value")),
    ("int_attr", AttributeValue::int(42L)),
    ("float_attr", AttributeValue::float(3.14)),
    ("bool_attr", AttributeValue::bool(true)),
    ("array_attr", AttributeValue::array_string(["a", "b", "c"]))
  ]
  
  // åºåˆ—åŒ–å±æ€§æ•°ç»„
  let serialized_attrs = []
  for attr in attributes {
    let serialized = match attr.1 {
      StringValue(s) => "string:" + s
      IntValue(i) => "int:" + i.to_string()
      FloatValue(f) => "float:" + f.to_string()
      BoolValue(b) => "bool:" + (if b { "true" } else { "false" })
      ArrayStringValue(arr) => "array:[" + arr.join(",") + "]"
      _ => "unknown"
    }
    serialized_attrs.push(attr.0 + "=" + serialized)
  }
  
  let serialized_string = serialized_attrs.join("|")
  
  // éªŒè¯åºåˆ—åŒ–ç»“æœ
  assert_eq(serialized_string.contains("string_attr=string:test_value"), true)
  assert_eq(serialized_string.contains("int_attr=int:42"), true)
  assert_eq(serialized_string.contains("float_attr=float:3.14"), true)
  assert_eq(serialized_string.contains("bool_attr=bool:true"), true)
  assert_eq(serialized_string.contains("array_attr=array:[a,b,c]"), true)
  
  // éªŒè¯ååºåˆ—åŒ–
  let deserialized_parts = serialized_string.split("|")
  assert_eq(deserialized_parts.length(), 5)
}

test "cross_format_serialization" {
  // æµ‹è¯•è·¨æ ¼å¼åºåˆ—åŒ–ï¼ˆJSONåˆ°XMLï¼‰
  
  let log_record = {
    "timestamp": "2023-01-01T12:00:00Z",
    "severity": "INFO",
    "message": "User login successful",
    "trace_id": "1234567890abcdef1234567890abcdef",
    "span_id": "fedcba0987654321",
    "attributes": [
      {"key": "user.id", "value": "user123"},
      {"key": "ip.address", "value": "192.168.1.100"}
    ]
  }
  
  // JSONæ ¼å¼
  let json_format = "{"
    + "\"timestamp\":\"" + log_record["timestamp"] + "\","
    + "\"severity\":\"" + log_record["severity"] + "\","
    + "\"message\":\"" + log_record["message"] + "\""
    + "}"
  
  // XMLæ ¼å¼
  let xml_format = "<log>"
    + "<timestamp>" + log_record["timestamp"] + "</timestamp>"
    + "<severity>" + log_record["severity"] + "</severity>"
    + "<message>" + log_record["message"] + "</message>"
    + "</log>"
  
  // éªŒè¯ä¸¤ç§æ ¼å¼
  assert_eq(json_format.has_prefix("{"), true)
  assert_eq(json_format.has_suffix("}"), true)
  assert_eq(json_format.contains("timestamp"), true)
  assert_eq(json_format.contains("severity"), true)
  
  assert_eq(xml_format.has_prefix("<log>"), true)
  assert_eq(xml_format.has_suffix("</log>"), true)
  assert_eq(xml_format.contains("<timestamp>"), true)
  assert_eq(xml_format.contains("<severity>"), true)
  
  // éªŒè¯æ•°æ®ä¸€è‡´æ€§
  assert_eq(json_format.contains("2023-01-01T12:00:00Z"), true)
  assert_eq(xml_format.contains("2023-01-01T12:00:00Z"), true)
  assert_eq(json_format.contains("INFO"), true)
  assert_eq(xml_format.contains("INFO"), true)
}

test "version_backward_compatibility" {
  // æµ‹è¯•ç‰ˆæœ¬å‘åå…¼å®¹æ€§
  
  let v1_format = {
    "trace_id": "0af7651916cd43dd8448eb211c80319c",
    "span_id": "b7ad6b7169203331",
    "operation": "http_request",  // v1å­—æ®µå
    "duration": 250               // v1å­—æ®µå
  }
  
  let v2_format = {
    "trace_id": "0af7651916cd43dd8448eb211c80319c",
    "span_id": "b7ad6b7169203331",
    "operation_name": "http_request",  // v2å­—æ®µå
    "duration_ms": 250,               // v2å­—æ®µå
    "status": "OK"                     // v2æ–°å¢å­—æ®µ
  }
  
  // æ¨¡æ‹Ÿv1åˆ°v2çš„è½¬æ¢
  let field_mappings = [
    ("operation", "operation_name"),
    ("duration", "duration_ms")
  ]
  
  let converted_v1_to_v2 = []
  for v1_field in v1_format.keys() {
    let mapped_field = field_mappings.find(fn(mapping) { mapping.0 == v1_field })
    match mapped_field {
      Some(mapping) => {
        converted_v1_to_v2.push((mapping.1, v1_format[v1_field]))
      }
      None => {
        // æœªæ˜ å°„çš„å­—æ®µä¿æŒä¸å˜
        if v1_field != "operation" and v1_field != "duration" {
          converted_v1_to_v2.push((v1_field, v1_format[v1_field]))
        }
      }
    }
  }
  
  // éªŒè¯è½¬æ¢ç»“æœ
  let has_operation_name = converted_v1_to_v2.any(fn(field) { field.0 == "operation_name" })
  let has_duration_ms = converted_v1_to_v2.any(fn(field) { field.0 == "duration_ms" })
  
  assert_eq(has_operation_name, true)
  assert_eq(has_duration_ms, true)
  assert_eq(converted_v1_to_v2.length(), 2)
}

test "binary_serialization_efficiency" {
  // æµ‹è¯•äºŒè¿›åˆ¶åºåˆ—åŒ–æ•ˆç‡
  
  let large_dataset = []
  for i = 0; i < 1000; i = i + 1 {
    large_dataset.push({
      "id": "item_" + i.to_string(),
      "value": i * 2,
      "timestamp": "2023-01-01T00:00:" + (if i < 10 { "0" } else { "" }) + i.to_string() + "Z"
    })
  }
  
  // æ–‡æœ¬åºåˆ—åŒ–ï¼ˆæ¨¡æ‹Ÿï¼‰
  let text_serialized = large_dataset.map(fn(item) {
    item["id"] + "|" + item["value"].to_string() + "|" + item["timestamp"]
  }).join("\n")
  
  // äºŒè¿›åˆ¶åºåˆ—åŒ–ï¼ˆæ¨¡æ‹Ÿå‹ç¼©ï¼‰
  let binary_compressed = text_serialized.replace("item_", "i_")
    .replace("2023-01-01T00:00:", "T")
    .replace("Z", "")
  
  // éªŒè¯å‹ç¼©æ•ˆç‡
  let text_size = text_serialized.length()
  let binary_size = binary_compressed.length()
  let compression_ratio = binary_size.to_double() / text_size.to_double()
  
  assert_eq(binary_size < text_size, true)
  assert_eq(compression_ratio < 1.0, true)
  assert_eq(compression_ratio > 0.5, true)  // å‹ç¼©ç‡ä¸è¶…è¿‡50%
  
  // éªŒè¯æ•°æ®å®Œæ•´æ€§
  assert_eq(binary_compressed.contains("i_0"), true)
  assert_eq(binary_compressed.contains("T00Z"), true)
}

test "serialization_error_handling" {
  // æµ‹è¯•åºåˆ—åŒ–é”™è¯¯å¤„ç†
  
  let problematic_data = [
    ("null_value", null),
    ("circular_reference", "self"),
    ("very_long_string", "a" * 1000000),
    ("special_chars", "ç‰¹æ®Šå­—ç¬¦ğŸš€\n\t\r"),
    ("unicode_data", "Unicodeæµ‹è¯•: ä¸­æ–‡, Ã±Ã¡Ã©Ã­Ã³Ãº, Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
  ]
  
  let serialization_results = []
  
  for data in problematic_data {
    let can_serialize = true
    let error_message = ""
    
    match data.0 {
      "null_value" => {
        // æ£€æŸ¥nullå€¼å¤„ç†
        can_serialize = false
        error_message = "Null values not supported"
      }
      "circular_reference" => {
        // æ£€æŸ¥å¾ªç¯å¼•ç”¨
        if data.1 == "self" {
          can_serialize = false
          error_message = "Circular reference detected"
        }
      }
      "very_long_string" => {
        // æ£€æŸ¥å­—ç¬¦ä¸²é•¿åº¦é™åˆ¶
        if data.1.length() > 100000 {
          can_serialize = false
          error_message = "String too long"
        }
      }
      "special_chars" => {
        // ç‰¹æ®Šå­—ç¬¦åº”è¯¥å¯ä»¥åºåˆ—åŒ–
        can_serialize = true
      }
      "unicode_data" => {
        // Unicodeåº”è¯¥å¯ä»¥åºåˆ—åŒ–
        can_serialize = true
      }
      _ => {
        can_serialize = false
        error_message = "Unknown data type"
      }
    }
    
    serialization_results.push((data.0, can_serialize, error_message))
  }
  
  // éªŒè¯é”™è¯¯å¤„ç†
  assert_eq(serialization_results.length(), 5)
  
  let successful_serializations = serialization_results.filter(fn(result) { result.1 })
  let failed_serializations = serialization_results.filter(fn(result) { not result.1 })
  
  assert_eq(successful_serializations.length(), 2)  // special_chars, unicode_data
  assert_eq(failed_serializations.length(), 3)     // null_value, circular_reference, very_long_string
}