// é¥æµ‹æ•°æ®å‹ç¼©æµ‹è¯•ç”¨ä¾‹

test "telemetry_compression_basic_gzip" {
  // æµ‹è¯•é¥æµ‹æ•°æ®åŸºæœ¬GZIPå‹ç¼©
  
  let original_data = "trace_id:0af7651916cd43dd8448eb211c80319c,span_id:b7ad6b7169203331,operation:GET/api/users,duration:120.5,status:success,timestamp:1640995200000"
  
  // éªŒè¯åŸå§‹æ•°æ®
  assert_eq(original_data.length(), 95)
  assert_eq(original_data.contains("trace_id"), true)
  assert_eq(original_data.contains("span_id"), true)
  assert_eq(original_data.contains("operation"), true)
  assert_eq(original_data.contains("duration"), true)
  assert_eq(original_data.contains("status"), true)
  assert_eq(original_data.contains("timestamp"), true)
  
  // æ¨¡æ‹ŸGZIPå‹ç¼©ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…ä½¿ç”¨ä¼šè°ƒç”¨å‹ç¼©åº“ï¼‰
  let compression_algorithm = "gzip"
  let compression_level = 6
  let original_size = original_data.length()
  
  // æ¨¡æ‹Ÿå‹ç¼©åçš„æ•°æ®ï¼ˆå®é™…å‹ç¼©ç‡å–å†³äºæ•°æ®å†…å®¹ï¼‰
  let compression_ratio = 0.3 // 70%å‹ç¼©ç‡
  let compressed_size = (original_size.to_double() * compression_ratio).to_int()
  
  // éªŒè¯å‹ç¼©å‚æ•°
  assert_eq(compression_algorithm, "gzip")
  assert_eq(compression_level, 6)
  assert_eq(original_size, 95)
  assert_eq(compression_ratio, 0.3)
  assert_eq(compressed_size, 28) // 95 * 0.3 = 28.5 -> 28
  
  // éªŒè¯å‹ç¼©æ•ˆæœ
  assert_eq(compressed_size < original_size, true)
  assert_eq(compressed_size > 0, true)
  
  // è®¡ç®—å‹ç¼©ç»Ÿè®¡
  let space_saved = original_size - compressed_size
  let space_saved_percentage = (space_saved.to_double() / original_size.to_double()) * 100.0
  
  // éªŒè¯å‹ç¼©ç»Ÿè®¡
  assert_eq(space_saved, 67) // 95 - 28
  assert_eq(space_saved_percentage, 70.52631578947368) // (67 / 95) * 100
  
  // åˆ›å»ºå‹ç¼©æŠ¥å‘Š
  let compression_report = [
    ("algorithm", compression_algorithm),
    ("level", compression_level.to_string()),
    ("original_size", original_size.to_string()),
    ("compressed_size", compressed_size.to_string()),
    ("compression_ratio", compression_ratio.to_string()),
    ("space_saved", space_saved.to_string()),
    ("space_saved_percentage", space_saved_percentage.to_string())
  ]
  
  // éªŒè¯å‹ç¼©æŠ¥å‘Š
  assert_eq(compression_report.length(), 7)
  assert_eq(compression_report[0].0, "algorithm")
  assert_eq(compression_report[0].1, "gzip")
  assert_eq(compression_report[1].0, "level")
  assert_eq(compression_report[1].1, "6")
  assert_eq(compression_report[2].0, "original_size")
  assert_eq(compression_report[2].1, "95")
  assert_eq(compression_report[3].0, "compressed_size")
  assert_eq(compression_report[3].1, "28")
}

test "telemetry_compression_batch_processing" {
  // æµ‹è¯•é¥æµ‹æ•°æ®æ‰¹å¤„ç†å‹ç¼©
  
  let telemetry_batch = [
    "trace_id:0af7651916cd43dd8448eb211c80319c,span_id:b7ad6b7169203331,operation:GET/api/users,duration:120.5",
    "trace_id:0af7651916cd43dd8448eb211c80319c,span_id:c7ad6b7169203331,operation:POST/api/orders,duration:250.3",
    "trace_id:0af7651916cd43dd8448eb211c80319c,span_id:d7ad6b7169203331,operation:DatabaseQuery,duration:5000.0",
    "trace_id:0af7651916cd43dd8448eb211c80319c,span_id:e7ad6b7169203331,operation:CacheLookup,duration:5.2",
    "trace_id:0af7651916cd43dd8448eb211c80319c,span_id:f7ad6b7169203331,operation:AuthValidation,duration:15.3"
  ]
  
  // éªŒè¯é¥æµ‹æ‰¹æ¬¡
  assert_eq(telemetry_batch.length(), 5)
  assert_eq(telemetry_batch[0].contains("GET/api/users"), true)
  assert_eq(telemetry_batch[1].contains("POST/api/orders"), true)
  assert_eq(telemetry_batch[2].contains("DatabaseQuery"), true)
  assert_eq(telemetry_batch[3].contains("CacheLookup"), true)
  assert_eq(telemetry_batch[4].contains("AuthValidation"), true)
  
  // è®¡ç®—æ‰¹æ¬¡æ€»å¤§å°
  let mut batch_size = 0
  let mut i = 0
  
  while i < telemetry_batch.length() {
    batch_size = batch_size + telemetry_batch[i].length()
    i = i + 1
  }
  
  // éªŒè¯æ‰¹æ¬¡å¤§å°
  assert_eq(batch_size, 375) // æ‰€æœ‰æ¡ç›®çš„æ€»é•¿åº¦
  
  // æ¨¡æ‹Ÿæ‰¹å¤„ç†å‹ç¼©ç­–ç•¥
  let compression_strategies = [
    ("individual", 0.4), // å•ç‹¬å‹ç¼©æ¯ä¸ªæ¡ç›®
    ("combined", 0.25), // åˆå¹¶æ‰€æœ‰æ¡ç›®åå‹ç¼©
    ("hybrid", 0.3) // æ··åˆç­–ç•¥
  ]
  
  // éªŒè¯å‹ç¼©ç­–ç•¥
  assert_eq(compression_strategies.length(), 3)
  assert_eq(compression_strategies[0].0, "individual")
  assert_eq(compression_strategies[0].1, 0.4)
  assert_eq(compression_strategies[1].0, "combined")
  assert_eq(compression_strategies[1].1, 0.25)
  assert_eq(compression_strategies[2].0, "hybrid")
  assert_eq(compression_strategies[2].1, 0.3)
  
  // è®¡ç®—ä¸åŒç­–ç•¥çš„å‹ç¼©ç»“æœ
  let compression_results = []
  i = 0
  
  while i < compression_strategies.length() {
    let strategy = compression_strategies[i].0
    let ratio = compression_strategies[i].1
    let compressed_size = (batch_size.to_double() * ratio).to_int()
    
    compression_results.push((strategy, batch_size, compressed_size, ratio))
    i = i + 1
  }
  
  // éªŒè¯å‹ç¼©ç»“æœ
  assert_eq(compression_results.length(), 3)
  assert_eq(compression_results[0].0, "individual")
  assert_eq(compression_results[0].2, 150) // 375 * 0.4
  assert_eq(compression_results[1].0, "combined")
  assert_eq(compression_results[1].2, 93) // 375 * 0.25 = 93.75 -> 93
  assert_eq(compression_results[2].0, "hybrid")
  assert_eq(compression_results[2].2, 112) // 375 * 0.3 = 112.5 -> 112
  
  // éªŒè¯æœ€ä½³å‹ç¼©ç­–ç•¥
  let mut best_strategy = ""
  let mut best_size = batch_size
  i = 0
  
  while i < compression_results.length() {
    if compression_results[i].2 < best_size {
      best_size = compression_results[i].2
      best_strategy = compression_results[i].0
    }
    i = i + 1
  }
  
  // éªŒè¯æœ€ä½³ç­–ç•¥
  assert_eq(best_strategy, "combined") // åˆå¹¶å‹ç¼©æ•ˆæœæœ€å¥½
  assert_eq(best_size, 93)
}

test "telemetry_compression_adaptive_level" {
  // æµ‹è¯•é¥æµ‹æ•°æ®è‡ªé€‚åº”å‹ç¼©çº§åˆ«
  
  let data_sizes = [100, 500, 1000, 5000, 10000, 50000]
  let expected_levels = [1, 3, 6, 6, 9, 9] // æ ¹æ®æ•°æ®å¤§å°è‡ªé€‚åº”è°ƒæ•´
  
  // éªŒè¯æ•°æ®å¤§å°æ•°ç»„
  assert_eq(data_sizes.length(), 6)
  assert_eq(data_sizes[0], 100)
  assert_eq(data_sizes[5], 50000)
  
  // éªŒè¯æœŸæœ›çº§åˆ«æ•°ç»„
  assert_eq(expected_levels.length(), 6)
  assert_eq(expected_levels[0], 1) // å°æ•°æ®ä½¿ç”¨ä½çº§åˆ«
  assert_eq(expected_levels[5], 9) // å¤§æ•°æ®ä½¿ç”¨é«˜çº§åˆ«
  
  // æ¨¡æ‹Ÿè‡ªé€‚åº”å‹ç¼©çº§åˆ«é€‰æ‹©
  let adaptive_levels = []
  let mut i = 0
  
  while i < data_sizes.length() {
    let size = data_sizes[i]
    let mut level = 1
    
    if size < 200 {
      level = 1
    } else if size < 1000 {
      level = 3
    } else if size < 10000 {
      level = 6
    } else {
      level = 9
    }
    
    adaptive_levels.push((size, level))
    i = i + 1
  }
  
  // éªŒè¯è‡ªé€‚åº”çº§åˆ«
  assert_eq(adaptive_levels.length(), 6)
  assert_eq(adaptive_levels[0].0, 100)
  assert_eq(adaptive_levels[0].1, 1)
  assert_eq(adaptive_levels[1].0, 500)
  assert_eq(adaptive_levels[1].1, 3)
  assert_eq(adaptive_levels[2].0, 1000)
  assert_eq(adaptive_levels[2].1, 6)
  assert_eq(adaptive_levels[5].0, 50000)
  assert_eq(adaptive_levels[5].1, 9)
  
  // æ¨¡æ‹Ÿä¸åŒçº§åˆ«çš„å‹ç¼©æ•ˆæœ
  let level_ratios = [
    (1, 0.8),   // çº§åˆ«1ï¼š20%å‹ç¼©ç‡
    (3, 0.6),   // çº§åˆ«3ï¼š40%å‹ç¼©ç‡
    (6, 0.4),   // çº§åˆ«6ï¼š60%å‹ç¼©ç‡
    (9, 0.25)   // çº§åˆ«9ï¼š75%å‹ç¼©ç‡
  ]
  
  // éªŒè¯çº§åˆ«æ¯”ç‡
  assert_eq(level_ratios.length(), 4)
  assert_eq(level_ratios[0].0, 1)
  assert_eq(level_ratios[0].1, 0.8)
  assert_eq(level_ratios[3].0, 9)
  assert_eq(level_ratios[3].1, 0.25)
  
  // è®¡ç®—è‡ªé€‚åº”å‹ç¼©çš„æ€§èƒ½
  let adaptive_performance = []
  i = 0
  
  while i < adaptive_levels.length() {
    let size = adaptive_levels[i].0
    let level = adaptive_levels[i].1
    
    // æŸ¥æ‰¾å¯¹åº”çº§åˆ«çš„å‹ç¼©æ¯”
    let mut ratio = 0.8 // é»˜è®¤çº§åˆ«1
    let mut j = 0
    
    while j < level_ratios.length() {
      if level_ratios[j].0 == level {
        ratio = level_ratios[j].1
        break
      }
      j = j + 1
    }
    
    let compressed_size = (size.to_double() * ratio).to_int()
    let compression_time = level.to_double() * 10.0 // æ›´é«˜çº§åˆ«éœ€è¦æ›´å¤šæ—¶é—´
    
    adaptive_performance.push((size, level, compressed_size, compression_time))
    i = i + 1
  }
  
  // éªŒè¯è‡ªé€‚åº”æ€§èƒ½
  assert_eq(adaptive_performance.length(), 6)
  assert_eq(adaptive_performance[0].0, 100)
  assert_eq(adaptive_performance[0].1, 1)
  assert_eq(adaptive_performance[0].2, 80) // 100 * 0.8
  assert_eq(adaptive_performance[0].3, 10.0) // 1 * 10
  assert_eq(adaptive_performance[5].0, 50000)
  assert_eq(adaptive_performance[5].1, 9)
  assert_eq(adaptive_performance[5].2, 12500) // 50000 * 0.25
  assert_eq(adaptive_performance[5].3, 90.0) // 9 * 10
}

test "telemetry_compression_format_comparison" {
  // æµ‹è¯•é¥æµ‹æ•°æ®å‹ç¼©æ ¼å¼æ¯”è¾ƒ
  
  let test_data = "trace_id:0af7651916cd43dd8448eb211c80319c,span_id:b7ad6b7169203331,operation:GET/api/users,duration:120.5,status:success,timestamp:1640995200000,attributes:service.name:api-gateway,service.version:1.2.3,environment:production"
  
  // éªŒè¯æµ‹è¯•æ•°æ®
  assert_eq(test_data.length(), 175)
  assert_eq(test_data.contains("trace_id"), true)
  assert_eq(test_data.contains("operation"), true)
  assert_eq(test_data.contains("attributes"), true)
  
  // ä¸åŒå‹ç¼©æ ¼å¼çš„æ€§èƒ½ç‰¹å¾
  let compression_formats = [
    ("gzip", 6, 0.3, 15.0),      // æ ¼å¼, çº§åˆ«, å‹ç¼©æ¯”, å‹ç¼©æ—¶é—´(ms)
    ("deflate", 6, 0.35, 12.0),
    ("lz4", 1, 0.5, 2.0),
    ("zstd", 3, 0.25, 8.0),
    ("brotli", 4, 0.2, 25.0)
  ]
  
  // éªŒè¯å‹ç¼©æ ¼å¼
  assert_eq(compression_formats.length(), 5)
  assert_eq(compression_formats[0].0, "gzip")
  assert_eq(compression_formats[0].1, 6)
  assert_eq(compression_formats[0].2, 0.3)
  assert_eq(compression_formats[0].3, 15.0)
  assert_eq(compression_formats[4].0, "brotli")
  assert_eq(compression_formats[4].1, 4)
  assert_eq(compression_formats[4].2, 0.2)
  assert_eq(compression_formats[4].3, 25.0)
  
  // è®¡ç®—æ¯ç§æ ¼å¼çš„å‹ç¼©ç»“æœ
  let format_results = []
  let mut i = 0
  
  while i < compression_formats.length() {
    let format = compression_formats[i].0
    let level = compression_formats[i].1
    let ratio = compression_formats[i].2
    let compression_time = compression_formats[i].3
    
    let original_size = test_data.length()
    let compressed_size = (original_size.to_double() * ratio).to_int()
    let space_saved = original_size - compressed_size
    
    format_results.push((
      format,
      level,
      original_size,
      compressed_size,
      space_saved,
      compression_time
    ))
    
    i = i + 1
  }
  
  // éªŒè¯æ ¼å¼ç»“æœ
  assert_eq(format_results.length(), 5)
  assert_eq(format_results[0].0, "gzip")
  assert_eq(format_results[0].3, 52) // 175 * 0.3 = 52.5 -> 52
  assert_eq(format_results[0].4, 123) // 175 - 52
  assert_eq(format_results[0].5, 15.0)
  assert_eq(format_results[4].0, "brotli")
  assert_eq(format_results[4].3, 35) // 175 * 0.2 = 35
  assert_eq(format_results[4].4, 140) // 175 - 35
  assert_eq(format_results[4].5, 25.0)
  
  // æŒ‰å‹ç¼©ç‡æ’åº
  let mut sorted_by_ratio = format_results
  let mut j = 0
  
  while j < sorted_by_ratio.length() - 1 {
    let mut k = j + 1
    while k < sorted_by_ratio.length() {
      if sorted_by_ratio[j].3 > sorted_by_ratio[k].3 {
        let temp = sorted_by_ratio[j]
        sorted_by_ratio[j] = sorted_by_ratio[k]
        sorted_by_ratio[k] = temp
      }
      k = k + 1
    }
    j = j + 1
  }
  
  // éªŒè¯æ’åºç»“æœï¼ˆæŒ‰å‹ç¼©åå¤§å°å‡åºï¼‰
  assert_eq(sorted_by_ratio[0].0, "brotli") // æœ€å°å‹ç¼©å¤§å°
  assert_eq(sorted_by_ratio[0].3, 35)
  assert_eq(sorted_by_ratio[4].0, "lz4") // æœ€å¤§å‹ç¼©å¤§å°
  assert_eq(sorted_by_ratio[4].3, 87) // 175 * 0.5 = 87.5 -> 87
  
  // æŒ‰å‹ç¼©æ—¶é—´æ’åº
  let mut sorted_by_time = format_results
  j = 0
  
  while j < sorted_by_time.length() - 1 {
    let mut k = j + 1
    while k < sorted_by_time.length() {
      if sorted_by_time[j].5 > sorted_by_time[k].5 {
        let temp = sorted_by_time[j]
        sorted_by_time[j] = sorted_by_time[k]
        sorted_by_time[k] = temp
      }
      k = k + 1
    }
    j = j + 1
  }
  
  // éªŒè¯æ—¶é—´æ’åºç»“æœï¼ˆæŒ‰å‹ç¼©æ—¶é—´å‡åºï¼‰
  assert_eq(sorted_by_time[0].0, "lz4") // æœ€å¿«å‹ç¼©
  assert_eq(sorted_by_time[0].5, 2.0)
  assert_eq(sorted_by_time[4].0, "brotli") // æœ€æ…¢å‹ç¼©
  assert_eq(sorted_by_time[4].5, 25.0)
}

test "telemetry_compression_streaming" {
  // æµ‹è¯•é¥æµ‹æ•°æ®æµå¼å‹ç¼©
  
  let stream_data_chunks = [
    "chunk1:trace_id=0af7651916cd43dd8448eb211c80319c",
    "chunk2:span_id=b7ad6b7169203331",
    "chunk3:operation=GET/api/users",
    "chunk4:duration=120.5",
    "chunk5:status=success",
    "chunk6:timestamp=1640995200000"
  ]
  
  // éªŒè¯æµæ•°æ®å—
  assert_eq(stream_data_chunks.length(), 6)
  assert_eq(stream_data_chunks[0].contains("trace_id"), true)
  assert_eq(stream_data_chunks[1].contains("span_id"), true)
  assert_eq(stream_data_chunks[5].contains("timestamp"), true)
  
  // è®¡ç®—æµæ•°æ®æ€»å¤§å°
  let mut stream_size = 0
  let mut i = 0
  
  while i < stream_data_chunks.length() {
    stream_size = stream_size + stream_data_chunks[i].length()
    i = i + 1
  }
  
  // éªŒè¯æµå¤§å°
  assert_eq(stream_size, 138)
  
  // æµå¼å‹ç¼©ç­–ç•¥
  let streaming_strategies = [
    ("chunk_by_chunk", 0.6),    // é€å—å‹ç¼©
    ("windowed", 0.4),          // æ»‘åŠ¨çª—å£å‹ç¼©
    ("adaptive", 0.35)          // è‡ªé€‚åº”çª—å£å‹ç¼©
  ]
  
  // éªŒè¯æµå¼ç­–ç•¥
  assert_eq(streaming_strategies.length(), 3)
  assert_eq(streaming_strategies[0].0, "chunk_by_chunk")
  assert_eq(streaming_strategies[0].1, 0.6)
  assert_eq(streaming_strategies[1].0, "windowed")
  assert_eq(streaming_strategies[1].1, 0.4)
  assert_eq(streaming_strategies[2].0, "adaptive")
  assert_eq(streaming_strategies[2].1, 0.35)
  
  // æ¨¡æ‹Ÿæµå¼å‹ç¼©è¿‡ç¨‹
  let streaming_results = []
  i = 0
  
  while i < streaming_strategies.length() {
    let strategy = streaming_strategies[i].0
    let ratio = streaming_strategies[i].1
    
    // æ¨¡æ‹Ÿæµå¼å‹ç¼©çš„å†…å­˜ä½¿ç”¨
    let memory_usage = match strategy {
      "chunk_by_chunk" => stream_size / stream_data_chunks.length(), // æ¯æ¬¡åªå¤„ç†ä¸€ä¸ªå—
      "windowed" => stream_size / 2, // æ»‘åŠ¨çª—å£ä½¿ç”¨ä¸€åŠå†…å­˜
      "adaptive" => stream_size / 3, // è‡ªé€‚åº”ä½¿ç”¨ä¸‰åˆ†ä¹‹ä¸€å†…å­˜
      _ => stream_size
    }
    
    let compressed_size = (stream_size.to_double() * ratio).to_int()
    let processing_time = match strategy {
      "chunk_by_chunk" => 5.0,
      "windowed" => 8.0,
      "adaptive" => 10.0,
      _ => 15.0
    }
    
    streaming_results.push((
      strategy,
      stream_size,
      compressed_size,
      memory_usage,
      processing_time
    ))
    
    i = i + 1
  }
  
  // éªŒè¯æµå¼å‹ç¼©ç»“æœ
  assert_eq(streaming_results.length(), 3)
  assert_eq(streaming_results[0].0, "chunk_by_chunk")
  assert_eq(streaming_results[0].2, 82) // 138 * 0.6 = 82.8 -> 82
  assert_eq(streaming_results[0].3, 23) // 138 / 6 = 23
  assert_eq(streaming_results[0].4, 5.0)
  assert_eq(streaming_results[2].0, "adaptive")
  assert_eq(streaming_results[2].2, 48) // 138 * 0.35 = 48.3 -> 48
  assert_eq(streaming_results[2].3, 46) // 138 / 3 = 46
  assert_eq(streaming_results[2].4, 10.0)
  
  // éªŒè¯å†…å­˜æ•ˆç‡
  assert_eq(streaming_results[0].3 < streaming_results[1].3, true) // chunk_by_chunkå†…å­˜ä½¿ç”¨æœ€å°‘
  assert_eq(streaming_results[2].3 < streaming_results[1].3, true) // adaptiveæ¯”windowedå†…å­˜ä½¿ç”¨å°‘
  
  // éªŒè¯å‹ç¼©æ•ˆç‡
  assert_eq(streaming_results[2].2 < streaming_results[1].2, true) // adaptiveå‹ç¼©ç‡æœ€å¥½
  assert_eq(streaming_results[1].2 < streaming_results[0].2, true) // windowedæ¯”chunk_by_chunkå‹ç¼©ç‡å¥½
}

test "telemetry_compression_error_handling" {
  // æµ‹è¯•é¥æµ‹æ•°æ®å‹ç¼©é”™è¯¯å¤„ç†
  
  let test_cases = [
    ("empty_data", ""),                    // ç©ºæ•°æ®
    ("null_data", "null"),                 // nullæ•°æ®
    ("invalid_format", "{invalid json"),   // æ— æ•ˆæ ¼å¼
    ("too_large", "x" * 1000000),          // è¿‡å¤§æ•°æ®
    ("special_chars", "\x00\x01\x02"),     // ç‰¹æ®Šå­—ç¬¦
    ("unicode_data", "æµ‹è¯•æ•°æ®ğŸš€ğŸŒŸ")        // Unicodeæ•°æ®
  ]
  
  // éªŒè¯æµ‹è¯•ç”¨ä¾‹
  assert_eq(test_cases.length(), 6)
  assert_eq(test_cases[0].0, "empty_data")
  assert_eq(test_cases[0].1, "")
  assert_eq(test_cases[1].0, "null_data")
  assert_eq(test_cases[1].1, "null")
  assert_eq(test_cases[5].0, "unicode_data")
  assert_eq(test_cases[5].1, "æµ‹è¯•æ•°æ®ğŸš€ğŸŒŸ")
  
  // æ¨¡æ‹Ÿé”™è¯¯å¤„ç†
  let error_results = []
  let mut i = 0
  
  while i < test_cases.length() {
    let case_name = test_cases[i].0
    let data = test_cases[i].1
    
    let mut error_type = "none"
    let mut handled_successfully = true
    let mut fallback_used = false
    
    // æ¨¡æ‹Ÿä¸åŒçš„é”™è¯¯æƒ…å†µ
    if case_name == "empty_data" {
      if data.length() == 0 {
        error_type = "empty_input"
        handled_successfully = true
        fallback_used = false
      }
    } else if case_name == "null_data" {
      if data == "null" {
        error_type = "null_input"
        handled_successfully = true
        fallback_used = true
      }
    } else if case_name == "invalid_format" {
      if data.contains("invalid") {
        error_type = "format_error"
        handled_successfully = true
        fallback_used = true
      }
    } else if case_name == "too_large" {
      if data.length() > 100000 {
        error_type = "size_limit_exceeded"
        handled_successfully = true
        fallback_used = true
      }
    } else if case_name == "special_chars" {
      if data.contains("\x00") {
        error_type = "invalid_characters"
        handled_successfully = true
        fallback_used = true
      }
    } else if case_name == "unicode_data" {
      // Unicodeæ•°æ®åº”è¯¥æ­£å¸¸å¤„ç†
      error_type = "none"
      handled_successfully = true
      fallback_used = false
    }
    
    error_results.push((
      case_name,
      data.length(),
      error_type,
      handled_successfully,
      fallback_used
    ))
    
    i = i + 1
  }
  
  // éªŒè¯é”™è¯¯å¤„ç†ç»“æœ
  assert_eq(error_results.length(), 6)
  assert_eq(error_results[0].0, "empty_data")
  assert_eq(error_results[0].2, "empty_input")
  assert_eq(error_results[0].3, true)
  assert_eq(error_results[0].4, false)
  assert_eq(error_results[1].0, "null_data")
  assert_eq(error_results[1].2, "null_input")
  assert_eq(error_results[1].3, true)
  assert_eq(error_results[1].4, true)
  assert_eq(error_results[5].0, "unicode_data")
  assert_eq(error_results[5].2, "none")
  assert_eq(error_results[5].3, true)
  assert_eq(error_results[5].4, false)
  
  // ç»Ÿè®¡é”™è¯¯å¤„ç†ç»“æœ
  let mut total_cases = error_results.length()
  let mut successful_cases = 0
  let mut fallback_cases = 0
  let mut error_cases = 0
  
  i = 0
  while i < error_results.length() {
    if error_results[i].3 {
      successful_cases = successful_cases + 1
    }
    if error_results[i].4 {
      fallback_cases = fallback_cases + 1
    }
    if error_results[i].2 != "none" {
      error_cases = error_cases + 1
    }
    i = i + 1
  }
  
  // éªŒè¯ç»Ÿè®¡ç»“æœ
  assert_eq(successful_cases, total_cases) // æ‰€æœ‰æƒ…å†µéƒ½æˆåŠŸå¤„ç†
  assert_eq(fallback_cases, 4) // 4ä¸ªæƒ…å†µä½¿ç”¨äº†å›é€€
  assert_eq(error_cases, 5) // 5ä¸ªæƒ…å†µæœ‰é”™è¯¯ç±»å‹
  
  // åˆ›å»ºé”™è¯¯å¤„ç†æŠ¥å‘Š
  let error_report = [
    ("total_cases", total_cases.to_string()),
    ("successful_cases", successful_cases.to_string()),
    ("fallback_cases", fallback_cases.to_string()),
    ("error_cases", error_cases.to_string()),
    ("success_rate", (successful_cases.to_double() / total_cases.to_double()).to_string())
  ]
  
  // éªŒè¯é”™è¯¯å¤„ç†æŠ¥å‘Š
  assert_eq(error_report.length(), 5)
  assert_eq(error_report[0].0, "total_cases")
  assert_eq(error_report[0].1, "6")
  assert_eq(error_report[1].0, "successful_cases")
  assert_eq(error_report[1].1, "6")
  assert_eq(error_report[2].0, "fallback_cases")
  assert_eq(error_report[2].1, "4")
  assert_eq(error_report[3].0, "error_cases")
  assert_eq(error_report[3].1, "5")
  assert_eq(error_report[4].0, "success_rate")
  assert_eq(error_report[4].1, "1.0")
}

test "telemetry_compression_performance_benchmark" {
  // æµ‹è¯•é¥æµ‹æ•°æ®å‹ç¼©æ€§èƒ½åŸºå‡†
  
  let benchmark_datasets = [
    ("small_trace", 100, 10),      // æ•°æ®é›†åç§°, å¤§å°(å­—èŠ‚), æ¡ç›®æ•°
    ("medium_trace", 1000, 100),
    ("large_trace", 10000, 1000),
    ("batch_logs", 50000, 5000),
    ("metrics_bulk", 100000, 10000)
  ]
  
  // éªŒè¯åŸºå‡†æ•°æ®é›†
  assert_eq(benchmark_datasets.length(), 5)
  assert_eq(benchmark_datasets[0].0, "small_trace")
  assert_eq(benchmark_datasets[0].1, 100)
  assert_eq(benchmark_datasets[0].2, 10)
  assert_eq(benchmark_datasets[4].0, "metrics_bulk")
  assert_eq(benchmark_datasets[4].1, 100000)
  assert_eq(benchmark_datasets[4].2, 10000)
  
  // å‹ç¼©ç®—æ³•æ€§èƒ½ç‰¹å¾
  let algorithm_performance = [
    ("gzip", 1.0, 1.0, 1.0),      // ç®—æ³•, å‹ç¼©é€Ÿåº¦, è§£å‹é€Ÿåº¦, å‹ç¼©ç‡
    ("lz4", 3.0, 5.0, 0.7),
    ("zstd", 0.8, 2.0, 1.2),
    ("brotli", 0.3, 0.8, 1.5)
  ]
  
  // éªŒè¯ç®—æ³•æ€§èƒ½
  assert_eq(algorithm_performance.length(), 4)
  assert_eq(algorithm_performance[0].0, "gzip")
  assert_eq(algorithm_performance[0].1, 1.0) // åŸºå‡†é€Ÿåº¦
  assert_eq(algorithm_performance[0].2, 1.0)
  assert_eq(algorithm_performance[0].3, 1.0) // åŸºå‡†å‹ç¼©ç‡
  assert_eq(algorithm_performance[3].0, "brotli")
  assert_eq(algorithm_performance[3].1, 0.3) // æ…¢å‹ç¼©
  assert_eq(algorithm_performance[3].2, 0.8) // æ…¢è§£å‹
  assert_eq(algorithm_performance[3].3, 1.5) // é«˜å‹ç¼©ç‡
  
  // è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
  let benchmark_results = []
  let mut i = 0
  
  while i < benchmark_datasets.length() {
    let dataset_name = benchmark_datasets[i].0
    let dataset_size = benchmark_datasets[i].1
    let entry_count = benchmark_datasets[i].2
    
    let mut j = 0
    while j < algorithm_performance.length() {
      let algorithm = algorithm_performance[j].0
      let compression_speed = algorithm_performance[j].1
      let decompression_speed = algorithm_performance[j].2
      let compression_ratio = algorithm_performance[j].3
      
      // è®¡ç®—æ€§èƒ½æŒ‡æ ‡
      let base_compression_time = dataset_size.to_double() / 1000.0 // åŸºç¡€å‹ç¼©æ—¶é—´
      let compression_time = base_compression_time / compression_speed
      let decompression_time = base_compression_time / decompression_speed
      let compressed_size = (dataset_size.to_double() / compression_ratio).to_int()
      
      let throughput_mbps = (dataset_size.to_double() / 1024.0 / 1024.0) / (compression_time / 1000.0)
      
      benchmark_results.push((
        dataset_name,
        algorithm,
        dataset_size,
        compressed_size,
        compression_time,
        decompression_time,
        throughput_mbps
      ))
      
      j = j + 1
    }
    
    i = i + 1
  }
  
  // éªŒè¯åŸºå‡†æµ‹è¯•ç»“æœ
  assert_eq(benchmark_results.length(), 20) // 5ä¸ªæ•°æ®é›† * 4ä¸ªç®—æ³•
  assert_eq(benchmark_results[0].0, "small_trace")
  assert_eq(benchmark_results[0].1, "gzip")
  assert_eq(benchmark_results[0].2, 100)
  assert_eq(benchmark_results[0].3, 100) // 100 / 1.0
  assert_eq(benchmark_results[0].4, 0.1) // 100 / 1000 / 1.0
  assert_eq(benchmark_results[0].5, 0.1) // 100 / 1000 / 1.0
  
  // æ‰¾å‡ºæœ€ä½³æ€§èƒ½ç»„åˆ
  let mut best_compression_speed = 0.0
  let mut best_compression_algo = ""
  let mut best_compression_dataset = ""
  
  let mut best_throughput = 0.0
  let mut best_throughput_algo = ""
  let mut best_throughput_dataset = ""
  
  i = 0
  while i < benchmark_results.length() {
    let compression_time = benchmark_results[i].4
    let throughput = benchmark_results[i].6
    let dataset_name = benchmark_results[i].0
    let algorithm = benchmark_results[i].1
    
    if compression_time > 0.0 && (1.0 / compression_time) > best_compression_speed {
      best_compression_speed = 1.0 / compression_time
      best_compression_algo = algorithm
      best_compression_dataset = dataset_name
    }
    
    if throughput > best_throughput {
      best_throughput = throughput
      best_throughput_algo = algorithm
      best_throughput_dataset = dataset_name
    }
    
    i = i + 1
  }
  
  // éªŒè¯æœ€ä½³æ€§èƒ½
  assert_eq(best_compression_algo.length() > 0, true)
  assert_eq(best_compression_dataset.length() > 0, true)
  assert_eq(best_throughput_algo.length() > 0, true)
  assert_eq(best_throughput_dataset.length() > 0, true)
  assert_eq(best_compression_speed > 0.0, true)
  assert_eq(best_throughput > 0.0, true)
  
  // åˆ›å»ºæ€§èƒ½åŸºå‡†æŠ¥å‘Š
  let benchmark_report = [
    ("total_tests", benchmark_results.length().to_string()),
    ("best_compression_speed", best_compression_speed.to_string()),
    ("best_compression_algo", best_compression_algo),
    ("best_compression_dataset", best_compression_dataset),
    ("best_throughput", best_throughput.to_string()),
    ("best_throughput_algo", best_throughput_algo),
    ("best_throughput_dataset", best_throughput_dataset)
  ]
  
  // éªŒè¯åŸºå‡†æŠ¥å‘Š
  assert_eq(benchmark_report.length(), 7)
  assert_eq(benchmark_report[0].0, "total_tests")
  assert_eq(benchmark_report[0].1, "20")
  assert_eq(benchmark_report[1].0, "best_compression_speed")
  assert_eq(benchmark_report[1].1, best_compression_speed.to_string())
  assert_eq(benchmark_report[2].0, "best_compression_algo")
  assert_eq(benchmark_report[2].1, best_compression_algo)
}

test "telemetry_compression_memory_optimization" {
  // æµ‹è¯•é¥æµ‹æ•°æ®å‹ç¼©å†…å­˜ä¼˜åŒ–
  
  let memory_constraints = [
    ("low_memory", 10 * 1024 * 1024),      // 10MB
    ("medium_memory", 50 * 1024 * 1024),   // 50MB
    ("high_memory", 200 * 1024 * 1024)     // 200MB
  ]
  
  // éªŒè¯å†…å­˜çº¦æŸ
  assert_eq(memory_constraints.length(), 3)
  assert_eq(memory_constraints[0].0, "low_memory")
  assert_eq(memory_constraints[0].1, 10485760) // 10MB
  assert_eq(memory_constraints[2].0, "high_memory")
  assert_eq(memory_constraints[2].1, 209715200) // 200MB
  
  // å†…å­˜ä¼˜åŒ–ç­–ç•¥
  let memory_strategies = [
    ("streaming", 0.2, 0.8),      // ç­–ç•¥, å†…å­˜ä½¿ç”¨ç‡, å‹ç¼©ç‡
    ("chunked", 0.4, 0.7),
    ("adaptive", 0.6, 0.6),
    ("full_buffer", 1.0, 0.5)
  ]
  
  // éªŒè¯å†…å­˜ç­–ç•¥
  assert_eq(memory_strategies.length(), 4)
  assert_eq(memory_strategies[0].0, "streaming")
  assert_eq(memory_strategies[0].1, 0.2) // 20%å†…å­˜ä½¿ç”¨
  assert_eq(memory_strategies[0].2, 0.8) // 80%å‹ç¼©ç‡
  assert_eq(memory_strategies[3].0, "full_buffer")
  assert_eq(memory_strategies[3].1, 1.0) // 100%å†…å­˜ä½¿ç”¨
  assert_eq(memory_strategies[3].2, 0.5) // 50%å‹ç¼©ç‡
  
  // æµ‹è¯•ä¸åŒå†…å­˜çº¦æŸä¸‹çš„ä¼˜åŒ–
  let optimization_results = []
  let mut i = 0
  
  while i < memory_constraints.length() {
    let constraint_name = memory_constraints[i].0
    let max_memory = memory_constraints[i].1
    
    let mut j = 0
    while j < memory_strategies.length() {
      let strategy = memory_strategies[j].0
      let memory_usage_rate = memory_strategies[j].1
      let compression_ratio = memory_strategies[j].2
      
      let used_memory = (max_memory.to_double() * memory_usage_rate).to_int()
      let test_data_size = max_memory / 2 // æµ‹è¯•æ•°æ®ä¸ºæœ€å¤§å†…å­˜çš„ä¸€åŠ
      let compressed_size = (test_data_size.to_double() * compression_ratio).to_int()
      
      // æ£€æŸ¥æ˜¯å¦åœ¨å†…å­˜çº¦æŸå†…
      let within_constraint = used_memory <= max_memory
      
      optimization_results.push((
        constraint_name,
        strategy,
        max_memory,
        used_memory,
        test_data_size,
        compressed_size,
        within_constraint
      ))
      
      j = j + 1
    }
    
    i = i + 1
  }
  
  // éªŒè¯ä¼˜åŒ–ç»“æœ
  assert_eq(optimization_results.length(), 12) // 3ä¸ªçº¦æŸ * 4ä¸ªç­–ç•¥
  assert_eq(optimization_results[0].0, "low_memory")
  assert_eq(optimization_results[0].1, "streaming")
  assert_eq(optimization_results[0].3, 2097152) // 10MB * 0.2
  assert_eq(optimization_results[0].6, true) // åœ¨çº¦æŸå†…
  
  // æ‰¾å‡ºæ¯ä¸ªçº¦æŸä¸‹çš„æœ€ä½³ç­–ç•¥
  let best_strategies = []
  i = 0
  
  while i < memory_constraints.length() {
    let constraint_name = memory_constraints[i].0
    let mut best_compression = 0
    let mut best_strategy = ""
    let mut best_compressed_size = 0
    
    let mut j = 0
    while j < optimization_results.length() {
      if optimization_results[j].0 == constraint_name && optimization_results[j].6 {
        let compressed_size = optimization_results[j].5
        let test_size = optimization_results[j].4
        let compression_ratio = test_size - compressed_size
        
        if compression_ratio > best_compression {
          best_compression = compression_ratio
          best_strategy = optimization_results[j].1
          best_compressed_size = compressed_size
        }
      }
      j = j + 1
    }
    
    best_strategies.push((constraint_name, best_strategy, best_compressed_size))
    i = i + 1
  }
  
  // éªŒè¯æœ€ä½³ç­–ç•¥
  assert_eq(best_strategies.length(), 3)
  assert_eq(best_strategies[0].0, "low_memory")
  assert_eq(best_strategies[0].1, "streaming") // ä½å†…å­˜ä¸‹ä½¿ç”¨æµå¼å¤„ç†
  assert_eq(best_strategies[1].0, "medium_memory")
  assert_eq(best_strategies[1].1, "chunked") // ä¸­ç­‰å†…å­˜ä¸‹ä½¿ç”¨åˆ†å—å¤„ç†
  assert_eq(best_strategies[2].0, "high_memory")
  assert_eq(best_strategies[2].1, "adaptive") // é«˜å†…å­˜ä¸‹ä½¿ç”¨è‡ªé€‚åº”å¤„ç†
  
  // åˆ›å»ºå†…å­˜ä¼˜åŒ–æŠ¥å‘Š
  let memory_report = [
    ("constraints_tested", memory_constraints.length().to_string()),
    ("strategies_evaluated", memory_strategies.length().to_string()),
    ("total_combinations", optimization_results.length().to_string()),
    ("best_low_memory_strategy", best_strategies[0].1),
    ("best_medium_memory_strategy", best_strategies[1].1),
    ("best_high_memory_strategy", best_strategies[2].1)
  ]
  
  // éªŒè¯å†…å­˜ä¼˜åŒ–æŠ¥å‘Š
  assert_eq(memory_report.length(), 6)
  assert_eq(memory_report[0].0, "constraints_tested")
  assert_eq(memory_report[0].1, "3")
  assert_eq(memory_report[1].0, "strategies_evaluated")
  assert_eq(memory_report[1].1, "4")
  assert_eq(memory_report[2].0, "total_combinations")
  assert_eq(memory_report[2].1, "12")
  assert_eq(memory_report[3].0, "best_low_memory_strategy")
  assert_eq(memory_report[3].1, "streaming")
}