// é¥æµ‹æ•°æ®è¾¹ç•Œæ¡ä»¶å’Œæç«¯æƒ…å†µæµ‹è¯•ç”¨ä¾‹

test "telemetry_extreme_volume_handling" {
  // æµ‹è¯•æç«¯æ•°æ®é‡å¤„ç†
  
  let volume_limits = {
    "max_spans_per_second": 100000,
    "max_metrics_per_second": 50000,
    "max_log_entries_per_second": 200000,
    "max_attributes_per_span": 128,
    "max_events_per_span": 64
  }
  
  let extreme_load_scenarios = [
    {"scenario": "normal_load", "spans": 1000, "metrics": 500, "logs": 2000},
    {"scenario": "high_load", "spans": 50000, "metrics": 25000, "logs": 100000},
    {"scenario": "extreme_load", "spans": 150000, "metrics": 75000, "logs": 300000},
    {"scenario": "burst_load", "spans": 200000, "metrics": 100000, "logs": 500000}
  ]
  
  // éªŒè¯å·é™åˆ¶é…ç½®
  assert_eq(volume_limits["max_spans_per_second"], 100000)
  assert_eq(volume_limits["max_metrics_per_second"], 50000)
  assert_eq(volume_limits["max_log_entries_per_second"], 200000)
  
  // éªŒè¯æç«¯è´Ÿè½½åœºæ™¯
  assert_eq(extreme_load_scenarios.length(), 4)
  assert_eq(extreme_load_scenarios[2]["scenario"], "extreme_load")
  assert_eq(extreme_load_scenarios[2]["spans"], 150000)
  
  // æµ‹è¯•ç³»ç»Ÿåœ¨æç«¯è´Ÿè½½ä¸‹çš„è¡Œä¸º
  let mut system_responses = []
  let mut i = 0
  while i < extreme_load_scenarios.length() {
    let scenario = extreme_load_scenarios[i]
    let span_overflow = scenario["spans"] > volume_limits["max_spans_per_second"]
    let metric_overflow = scenario["metrics"] > volume_limits["max_metrics_per_second"]
    let log_overflow = scenario["logs"] > volume_limits["max_log_entries_per_second"]
    
    let response = {
      "scenario": scenario["scenario"],
      "spans_processed": if span_overflow { volume_limits["max_spans_per_second"] } else { scenario["spans"] },
      "metrics_processed": if metric_overflow { volume_limits["max_metrics_per_second"] } else { scenario["metrics"] },
      "logs_processed": if log_overflow { volume_limits["max_log_entries_per_second"] } else { scenario["logs"] },
      "spans_dropped": if span_overflow { scenario["spans"] - volume_limits["max_spans_per_second"] } else { 0 },
      "metrics_dropped": if metric_overflow { scenario["metrics"] - volume_limits["max_metrics_per_second"] } else { 0 },
      "logs_dropped": if log_overflow { scenario["logs"] - volume_limits["max_log_entries_per_second"] } else { 0 }
    }
    system_responses.push(response)
    i = i + 1
  }
  
  // éªŒè¯ç³»ç»Ÿå“åº”
  assert_eq(system_responses.length(), 4)
  assert_eq(system_responses[0]["scenario"], "normal_load")
  assert_eq(system_responses[0]["spans_processed"], 1000)  // æ­£å¸¸å¤„ç†
  assert_eq(system_responses[0]["spans_dropped"], 0)
  
  assert_eq(system_responses[2]["scenario"], "extreme_load")
  assert_eq(system_responses[2]["spans_processed"], 100000)  // é™åˆ¶å¤„ç†
  assert_eq(system_responses[2]["spans_dropped"], 50000)     // è¶…å‡ºéƒ¨åˆ†ä¸¢å¼ƒ
  
  assert_eq(system_responses[3]["scenario"], "burst_load")
  assert_eq(system_responses[3]["spans_processed"], 100000)  // é™åˆ¶å¤„ç†
  assert_eq(system_responses[3]["spans_dropped"], 100000)    // è¶…å‡ºéƒ¨åˆ†ä¸¢å¼ƒ
  
  // éªŒè¯ç³»ç»Ÿåœ¨æç«¯è´Ÿè½½ä¸‹çš„ç¨³å®šæ€§
  i = 0
  while i < system_responses.length() {
    let response = system_responses[i]
    assert_eq(response["spans_processed"] <= volume_limits["max_spans_per_second"], true)
    assert_eq(response["metrics_processed"] <= volume_limits["max_metrics_per_second"], true)
    assert_eq(response["logs_processed"] <= volume_limits["max_log_entries_per_second"], true)
    i = i + 1
  }
}

test "telemetry_extreme_attribute_sizes" {
  // æµ‹è¯•æç«¯å±æ€§å¤§å°å¤„ç†
  
  let attribute_limits = {
    "max_key_length": 256,
    "max_string_value_length": 4096,
    "max_array_length": 64,
    "max_nested_depth": 8
  }
  
  let extreme_attributes = [
    {"key": "normal_key", "value": "normal_value", "type": "string"},
    {"key": "a"*300, "value": "test", "type": "string"},  // è¶…é•¿é”®å
    {"key": "test", "value": "b"*5000, "type": "string"},  // è¶…é•¿å­—ç¬¦ä¸²å€¼
    {"key": "array_test", "value": ["item"] * 100, "type": "array"},  // è¶…é•¿æ•°ç»„
    {"key": "", "value": "empty_key", "type": "string"},  // ç©ºé”®å
    {"key": "null_value", "value": "", "type": "string"},  // ç©ºå€¼
    {"key": "unicode_test", "value": "æµ‹è¯•ğŸš€emoji", "type": "string"}  // Unicodeå­—ç¬¦
  ]
  
  // éªŒè¯å±æ€§é™åˆ¶
  assert_eq(attribute_limits["max_key_length"], 256)
  assert_eq(attribute_limits["max_string_value_length"], 4096)
  assert_eq(attribute_limits["max_array_length"], 64)
  
  // éªŒè¯æç«¯å±æ€§
  assert_eq(extreme_attributes.length(), 7)
  assert_eq(extreme_attributes[1]["key"].length(), 300)  // è¶…é•¿é”®å
  assert_eq(extreme_attributes[2]["value"].length(), 5000)  // è¶…é•¿å­—ç¬¦ä¸²å€¼
  
  // å¤„ç†æç«¯å±æ€§
  let mut processed_attributes = []
  let mut i = 0
  while i < extreme_attributes.length() {
    let attr = extreme_attributes[i]
    let mut processed = attr
    
    // å¤„ç†è¶…é•¿é”®å
    if attr["key"].length() > attribute_limits["max_key_length"] {
      processed["key"] = attr["key"].substring(0, attribute_limits["max_key_length"])
    }
    
    // å¤„ç†ç©ºé”®å
    if attr["key"].length() == 0 {
      processed["key"] = "empty_key_" + i.to_string()
    }
    
    // å¤„ç†è¶…é•¿å­—ç¬¦ä¸²å€¼
    if attr["type"] == "string" && attr["value"].length() > attribute_limits["max_string_value_length"] {
      processed["value"] = attr["value"].substring(0, attribute_limits["max_string_value_length"]) + "...[truncated]"
    }
    
    // å¤„ç†è¶…é•¿æ•°ç»„
    if attr["type"] == "array" && attr["value"].length() > attribute_limits["max_array_length"] {
      let truncated_array = []
      let mut j = 0
      while j < attribute_limits["max_array_length"] {
        truncated_array.push(attr["value"][j])
        j = j + 1
      }
      processed["value"] = truncated_array
    }
    
    processed_attributes.push(processed)
    i = i + 1
  }
  
  // éªŒè¯å¤„ç†åçš„å±æ€§
  assert_eq(processed_attributes.length(), 7)
  assert_eq(processed_attributes[1]["key"].length(), 256)  // æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
  assert_eq(processed_attributes[2]["value"].length(), 4096 + 13)  // æˆªæ–­å¹¶æ·»åŠ æ ‡è®°
  assert_eq(processed_attributes[3]["value"].length(), 64)  // æ•°ç»„æˆªæ–­
  assert_eq(processed_attributes[4]["key"].starts_with("empty_key_"), true)  // ç©ºé”®åå¤„ç†
  assert_eq(processed_attributes[6]["value"], "æµ‹è¯•ğŸš€emoji")  // Unicodeä¿æŒä¸å˜
  
  // éªŒè¯æ‰€æœ‰å¤„ç†åçš„å±æ€§éƒ½ç¬¦åˆé™åˆ¶
  i = 0
  while i < processed_attributes.length() {
    let attr = processed_attributes[i]
    assert_eq(attr["key"].length() > 0 && attr["key"].length() <= attribute_limits["max_key_length"], true)
    
    if attr["type"] == "string" {
      assert_eq(attr["value"].length() <= attribute_limits["max_string_value_length"] + 13, true)
    } else if attr["type"] == "array" {
      assert_eq(attr["value"].length() <= attribute_limits["max_array_length"], true)
    }
    i = i + 1
  }
}

test "telemetry_extreme_time_values" {
  // æµ‹è¯•æç«¯æ—¶é—´å€¼å¤„ç†
  
  let time_boundaries = {
    "min_timestamp": -9223372036854775808L,  // Long.MIN_VALUE
    "max_timestamp": 9223372036854775807L,   // Long.MAX_VALUE
    "unix_epoch": 0L,
    "y2038_boundary": 2147483647L,           // 2038å¹´é—®é¢˜
    "reasonable_min": 315576000000L,          // 1980-01-01
    "reasonable_max": 4102444800000L         // 2100-01-01
  }
  
  let extreme_time_values = [
    {"event_id": "evt_001", "timestamp": time_boundaries["min_timestamp"]},
    {"event_id": "evt_002", "timestamp": time_boundaries["max_timestamp"]},
    {"event_id": "evt_003", "timestamp": time_boundaries["unix_epoch"]},
    {"event_id": "evt_004", "timestamp": time_boundaries["y2038_boundary"]},
    {"event_id": "evt_005", "timestamp": time_boundaries["reasonable_min"]},
    {"event_id": "evt_006", "timestamp": time_boundaries["reasonable_max"]},
    {"event_id": "evt_007", "timestamp": -1L},  // è´Ÿæ—¶é—´æˆ³
    {"event_id": "evt_008", "timestamp": 18446744073709551615L}  // è¶…å¤§æ—¶é—´æˆ³ï¼ˆæº¢å‡ºï¼‰
  ]
  
  // éªŒè¯æ—¶é—´è¾¹ç•Œ
  assert_eq(time_boundaries["min_timestamp"], -9223372036854775808L)
  assert_eq(time_boundaries["max_timestamp"], 9223372036854775807L)
  assert_eq(time_boundaries["unix_epoch"], 0L)
  
  // éªŒè¯æç«¯æ—¶é—´å€¼
  assert_eq(extreme_time_values.length(), 8)
  assert_eq(extreme_time_values[0]["timestamp"], time_boundaries["min_timestamp"])
  assert_eq(extreme_time_values[1]["timestamp"], time_boundaries["max_timestamp"])
  
  // å¤„ç†æç«¯æ—¶é—´å€¼
  let mut processed_times = []
  let mut i = 0
  while i < extreme_time_values.length() {
    let event = extreme_time_values[i]
    let mut processed_timestamp = event["timestamp"]
    let mut time_status = "valid"
    
    // æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
    if event["timestamp"] < time_boundaries["reasonable_min"] || 
       event["timestamp"] > time_boundaries["reasonable_max"] {
      time_status = "out_of_bounds"
      
      // å°†æç«¯æ—¶é—´æˆ³è°ƒæ•´åˆ°åˆç†èŒƒå›´
      if event["timestamp"] < time_boundaries["reasonable_min"] {
        processed_timestamp = time_boundaries["reasonable_min"]
      } else if event["timestamp"] > time_boundaries["reasonable_max"] {
        processed_timestamp = time_boundaries["reasonable_max"]
      }
    }
    
    // æ£€æŸ¥æ˜¯å¦æ¥è¿‘è¾¹ç•Œå€¼
    if event["timestamp"] == time_boundaries["min_timestamp"] || 
       event["timestamp"] == time_boundaries["max_timestamp"] {
      time_status = "boundary_value"
    }
    
    processed_times.push({
      "event_id": event["event_id"],
      "original_timestamp": event["timestamp"],
      "processed_timestamp": processed_timestamp,
      "status": time_status
    })
    i = i + 1
  }
  
  // éªŒè¯å¤„ç†åçš„æ—¶é—´å€¼
  assert_eq(processed_times.length(), 8)
  assert_eq(processed_times[0]["status"], "boundary_value")
  assert_eq(processed_times[1]["status"], "boundary_value")
  assert_eq(processed_times[2]["status"], "valid")
  assert_eq(processed_times[6]["status"], "out_of_bounds")
  assert_eq(processed_times[7]["status"], "out_of_bounds")
  
  // éªŒè¯è°ƒæ•´åçš„æ—¶é—´æˆ³åœ¨åˆç†èŒƒå›´å†…
  i = 0
  while i < processed_times.length() {
    let processed = processed_times[i]
    assert_eq(processed["processed_timestamp"] >= time_boundaries["reasonable_min"], true)
    assert_eq(processed["processed_timestamp"] <= time_boundaries["reasonable_max"], true)
    i = i + 1
  }
  
  // éªŒè¯ç‰¹æ®Šæ—¶é—´æˆ³å¤„ç†
  assert_eq(processed_times[2]["processed_timestamp"], time_boundaries["unix_epoch"])
  assert_eq(processed_times[3]["processed_timestamp"], time_boundaries["y2038_boundary"])
  assert_eq(processed_times[6]["processed_timestamp"], time_boundaries["reasonable_min"])  // è´Ÿæ—¶é—´æˆ³è°ƒæ•´
}

test "telemetry_extreme_nesting_depth" {
  // æµ‹è¯•æç«¯åµŒå¥—æ·±åº¦å¤„ç†
  
  let nesting_limits = {
    "max_depth": 8,
    "max_object_size": 1024
  }
  
  // åˆ›å»ºæç«¯åµŒå¥—ç»“æ„
  let create_nested_structure = fn(depth: Int) -> Any {
    if depth <= 0 {
      "leaf_value"
    } else {
      {"nested": create_nested_structure(depth - 1), "level": depth}
    }
  }
  
  let extreme_structures = [
    {"name": "shallow", "depth": 2, "structure": create_nested_structure(2)},
    {"name": "normal", "depth": 5, "structure": create_nested_structure(5)},
    {"name": "deep", "depth": 10, "structure": create_nested_structure(10)},
    {"name": "extreme_deep", "depth": 20, "structure": create_nested_structure(20)}
  ]
  
  // éªŒè¯åµŒå¥—é™åˆ¶
  assert_eq(nesting_limits["max_depth"], 8)
  assert_eq(nesting_limits["max_object_size"], 1024)
  
  // éªŒè¯æç«¯ç»“æ„
  assert_eq(extreme_structures.length(), 4)
  assert_eq(extreme_structures[2]["name"], "deep")
  assert_eq(extreme_structures[2]["depth"], 10)
  
  // å¤„ç†æç«¯åµŒå¥—ç»“æ„
  let mut processed_structures = []
  let mut i = 0
  while i < extreme_structures.length() {
    let structure_info = extreme_structures[i]
    let mut processed_depth = structure_info["depth"]
    let mut processing_status = "valid"
    
    // æ£€æŸ¥åµŒå¥—æ·±åº¦
    if structure_info["depth"] > nesting_limits["max_depth"] {
      processing_status = "depth_limited"
      processed_depth = nesting_limits["max_depth"]
    }
    
    // åˆ›å»ºå¤„ç†åçš„ç»“æ„ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    let create_processed_structure = fn(depth: Int) -> Any {
      if depth <= 0 {
        {"value": "leaf", "truncated": false}
      } else if depth > nesting_limits["max_depth"] {
        {"value": "truncated_root", "truncated": true, "original_depth": structure_info["depth"]}
      } else {
        {"nested": create_processed_structure(depth - 1), "level": depth, "truncated": false}
      }
    }
    
    processed_structures.push({
      "name": structure_info["name"],
      "original_depth": structure_info["depth"],
      "processed_depth": processed_depth,
      "status": processing_status,
      "structure": create_processed_structure(processed_depth)
    })
    i = i + 1
  }
  
  // éªŒè¯å¤„ç†åçš„ç»“æ„
  assert_eq(processed_structures.length(), 4)
  assert_eq(processed_structures[0]["status"], "valid")
  assert_eq(processed_structures[1]["status"], "valid")
  assert_eq(processed_structures[2]["status"], "depth_limited")
  assert_eq(processed_structures[3]["status"], "depth_limited")
  
  // éªŒè¯æ·±åº¦é™åˆ¶
  i = 0
  while i < processed_structures.length() {
    let processed = processed_structures[i]
    assert_eq(processed["processed_depth"] <= nesting_limits["max_depth"], true)
    i = i + 1
  }
  
  assert_eq(processed_structures[2]["processed_depth"], 8)  // æ·±åº¦é™åˆ¶åˆ°8
  assert_eq(processed_structures[3]["processed_depth"], 8)  // æ·±åº¦é™åˆ¶åˆ°8
}

test "telemetry_extreme_concurrent_operations" {
  // æµ‹è¯•æç«¯å¹¶å‘æ“ä½œå¤„ç†
  
  let concurrency_limits = {
    "max_concurrent_spans": 10000,
    "max_concurrent_metrics": 5000,
    "max_concurrent_operations": 50000
  }
  
  let concurrent_scenarios = [
    {"scenario": "low_concurrency", "spans": 100, "metrics": 50, "operations": 500},
    {"scenario": "medium_concurrency", "spans": 1000, "metrics": 500, "operations": 5000},
    {"scenario": "high_concurrency", "spans": 5000, "metrics": 2500, "operations": 25000},
    {"scenario": "extreme_concurrency", "spans": 15000, "metrics": 7500, "operations": 75000},
    {"scenario": "overload_concurrency", "spans": 25000, "metrics": 12500, "operations": 125000}
  ]
  
  // éªŒè¯å¹¶å‘é™åˆ¶
  assert_eq(concurrency_limits["max_concurrent_spans"], 10000)
  assert_eq(concurrency_limits["max_concurrent_metrics"], 5000)
  assert_eq(concurrency_limits["max_concurrent_operations"], 50000)
  
  // éªŒè¯å¹¶å‘åœºæ™¯
  assert_eq(concurrent_scenarios.length(), 5)
  assert_eq(concurrent_scenarios[3]["scenario"], "extreme_concurrency")
  assert_eq(concurrent_scenarios[4]["scenario"], "overload_concurrency")
  
  // å¤„ç†æç«¯å¹¶å‘åœºæ™¯
  let mut concurrency_results = []
  let mut i = 0
  while i < concurrent_scenarios.length() {
    let scenario = concurrent_scenarios[i]
    
    // è®¡ç®—èµ„æºåˆ©ç”¨ç‡
    let span_utilization = scenario["spans"].to_double() / concurrency_limits["max_concurrent_spans"].to_double()
    let metric_utilization = scenario["metrics"].to_double() / concurrency_limits["max_concurrent_metrics"].to_double()
    let operation_utilization = scenario["operations"].to_double() / concurrency_limits["max_concurrent_operations"].to_double()
    
    // ç¡®å®šç³»ç»ŸçŠ¶æ€
    let system_status = if span_utilization > 1.0 || metric_utilization > 1.0 || operation_utilization > 1.0 {
      "overloaded"
    } else if span_utilization > 0.8 || metric_utilization > 0.8 || operation_utilization > 0.8 {
      "high_load"
    } else if span_utilization > 0.5 || metric_utilization > 0.5 || operation_utilization > 0.5 {
      "medium_load"
    } else {
      "low_load"
    }
    
    // è®¡ç®—æ‹’ç»çš„æ“ä½œæ•°
    let spans_rejected = if scenario["spans"] > concurrency_limits["max_concurrent_spans"] {
      scenario["spans"] - concurrency_limits["max_concurrent_spans"]
    } else {
      0
    }
    
    let metrics_rejected = if scenario["metrics"] > concurrency_limits["max_concurrent_metrics"] {
      scenario["metrics"] - concurrency_limits["max_concurrent_metrics"]
    } else {
      0
    }
    
    let operations_rejected = if scenario["operations"] > concurrency_limits["max_concurrent_operations"] {
      scenario["operations"] - concurrency_limits["max_concurrent_operations"]
    } else {
      0
    }
    
    concurrency_results.push({
      "scenario": scenario["scenario"],
      "system_status": system_status,
      "span_utilization": span_utilization,
      "metric_utilization": metric_utilization,
      "operation_utilization": operation_utilization,
      "spans_accepted": if spans_rejected > 0 { concurrency_limits["max_concurrent_spans"] } else { scenario["spans"] },
      "spans_rejected": spans_rejected,
      "metrics_accepted": if metrics_rejected > 0 { concurrency_limits["max_concurrent_metrics"] } else { scenario["metrics"] },
      "metrics_rejected": metrics_rejected,
      "operations_accepted": if operations_rejected > 0 { concurrency_limits["max_concurrent_operations"] } else { scenario["operations"] },
      "operations_rejected": operations_rejected
    })
    i = i + 1
  }
  
  // éªŒè¯å¹¶å‘å¤„ç†ç»“æœ
  assert_eq(concurrency_results.length(), 5)
  assert_eq(concurrency_results[0]["system_status"], "low_load")
  assert_eq(concurrency_results[1]["system_status"], "medium_load")
  assert_eq(concurrency_results[2]["system_status"], "high_load")
  assert_eq(concurrency_results[3]["system_status"], "overloaded")
  assert_eq(concurrency_results[4]["system_status"], "overloaded")
  
  // éªŒè¯æ‹’ç»æ“ä½œçš„è®¡ç®—
  assert_eq(concurrency_results[3]["spans_rejected"], 5000)    // 15000 - 10000
  assert_eq(concurrency_results[3]["metrics_rejected"], 2500)  // 7500 - 5000
  assert_eq(concurrency_results[3]["operations_rejected"], 25000) // 75000 - 50000
  
  assert_eq(concurrency_results[4]["spans_rejected"], 15000)   // 25000 - 10000
  assert_eq(concurrency_results[4]["metrics_rejected"], 7500)  // 12500 - 5000
  assert_eq(concurrency_results[4]["operations_rejected"], 75000) // 125000 - 50000
  
  // éªŒè¯ç³»ç»Ÿåœ¨æç«¯å¹¶å‘ä¸‹çš„ç¨³å®šæ€§
  i = 0
  while i < concurrency_results.length() {
    let result = concurrency_results[i]
    assert_eq(result["spans_accepted"] <= concurrency_limits["max_concurrent_spans"], true)
    assert_eq(result["metrics_accepted"] <= concurrency_limits["max_concurrent_metrics"], true)
    assert_eq(result["operations_accepted"] <= concurrency_limits["max_concurrent_operations"], true)
    i = i + 1
  }
}

test "telemetry_extreme_network_conditions" {
  // æµ‹è¯•æç«¯ç½‘ç»œæ¡ä»¶å¤„ç†
  
  let network_thresholds = {
    "max_latency_ms": 5000,
    "max_packet_loss_rate": 0.5,  // 50%
    "min_bandwidth_kbps": 10,
    "max_connection_failures": 10
  }
  
  let extreme_network_conditions = [
    {"condition": "perfect", "latency_ms": 10, "packet_loss": 0.0, "bandwidth_kbps": 10000, "failures": 0},
    {"condition": "good", "latency_ms": 50, "packet_loss": 0.001, "bandwidth_kbps": 5000, "failures": 1},
    {"condition": "poor", "latency_ms": 500, "packet_loss": 0.05, "bandwidth_kbps": 500, "failures": 3},
    {"condition": "bad", "latency_ms": 2000, "packet_loss": 0.2, "bandwidth_kbps": 100, "failures": 7},
    {"condition": "extreme", "latency_ms": 8000, "packet_loss": 0.6, "bandwidth_kbps": 5, "failures": 15},
    {"condition": "unusable", "latency_ms": 15000, "packet_loss": 0.8, "bandwidth_kbps": 1, "failures": 25}
  ]
  
  // éªŒè¯ç½‘ç»œé˜ˆå€¼
  assert_eq(network_thresholds["max_latency_ms"], 5000)
  assert_eq(network_thresholds["max_packet_loss_rate"], 0.5)
  assert_eq(network_thresholds["min_bandwidth_kbps"], 10)
  
  // éªŒè¯æç«¯ç½‘ç»œæ¡ä»¶
  assert_eq(extreme_network_conditions.length(), 6)
  assert_eq(extreme_network_conditions[4]["condition"], "extreme")
  assert_eq(extreme_network_conditions[5]["condition"], "unusable")
  
  // å¤„ç†æç«¯ç½‘ç»œæ¡ä»¶
  let mut network_responses = []
  let mut i = 0
  while i < extreme_network_conditions.length() {
    let condition = extreme_network_conditions[i]
    
    // è¯„ä¼°ç½‘ç»œè´¨é‡
    let latency_critical = condition["latency_ms"] > network_thresholds["max_latency_ms"]
    let loss_critical = condition["packet_loss"] > network_thresholds["max_packet_loss_rate"]
    let bandwidth_critical = condition["bandwidth_kbps"] < network_thresholds["min_bandwidth_kbps"]
    let failures_critical = condition["failures"] > network_thresholds["max_connection_failures"]
    
    // ç¡®å®šç½‘ç»œè´¨é‡ç­‰çº§
    let network_quality = if latency_critical || loss_critical || bandwidth_critical || failures_critical {
      "critical"
    } else if condition["latency_ms"] > 1000 || condition["packet_loss"] > 0.1 || condition["bandwidth_kbps"] < 100 {
      "poor"
    } else if condition["latency_ms"] > 100 || condition["packet_loss"] > 0.01 || condition["bandwidth_kbps"] < 1000 {
      "fair"
    } else {
      "good"
    }
    
    // ç¡®å®šé¥æµ‹ä¼ è¾“ç­–ç•¥
    let transmission_strategy = match network_quality {
      "critical" => "batch_and_compress",
      "poor" => "adaptive_sampling",
      "fair" => "normal_batching",
      _ => "real_time"
    }
    
    // è®¡ç®—é¢„æœŸæ•°æ®ä¸¢å¤±ç‡
    let expected_data_loss = if network_quality == "critical" {
      0.5 + condition["packet_loss"] * 0.5
    } else if network_quality == "poor" {
      0.1 + condition["packet_loss"] * 0.3
    } else {
      condition["packet_loss"] * 0.1
    }
    
    network_responses.push({
      "condition": condition["condition"],
      "network_quality": network_quality,
      "transmission_strategy": transmission_strategy,
      "expected_data_loss": expected_data_loss,
      "latency_acceptable": !latency_critical,
      "loss_acceptable": !loss_critical,
      "bandwidth_acceptable": !bandwidth_critical,
      "failures_acceptable": !failures_critical
    })
    i = i + 1
  }
  
  // éªŒè¯ç½‘ç»œå“åº”
  assert_eq(network_responses.length(), 6)
  assert_eq(network_responses[0]["network_quality"], "good")
  assert_eq(network_responses[1]["network_quality"], "fair")
  assert_eq(network_responses[2]["network_quality"], "poor")
  assert_eq(network_responses[3]["network_quality"], "poor")
  assert_eq(network_responses[4]["network_quality"], "critical")
  assert_eq(network_responses[5]["network_quality"], "critical")
  
  // éªŒè¯ä¼ è¾“ç­–ç•¥
  assert_eq(network_responses[0]["transmission_strategy"], "real_time")
  assert_eq(network_responses[2]["transmission_strategy"], "adaptive_sampling")
  assert_eq(network_responses[4]["transmission_strategy"], "batch_and_compress")
  
  // éªŒè¯æç«¯æ¡ä»¶ä¸‹çš„é€‚åº”æ€§
  assert_eq(network_responses[4]["latency_acceptable"], false)
  assert_eq(network_responses[4]["loss_acceptable"], false)
  assert_eq(network_responses[4]["bandwidth_acceptable"], false)
  assert_eq(network_responses[4]["failures_acceptable"], false)
  
  assert_eq(network_responses[5]["latency_acceptable"], false)
  assert_eq(network_responses[5]["loss_acceptable"], false)
  assert_eq(network_responses[5]["bandwidth_acceptable"], false)
  assert_eq(network_responses[5]["failures_acceptable"], false)
  
  // éªŒè¯æ•°æ®ä¸¢å¤±é¢„æœŸ
  assert_eq(network_responses[0]["expected_data_loss"], 0.0)
  assert_eq(network_responses[2]["expected_data_loss"] > 0.1, true)
  assert_eq(network_responses[4]["expected_data_loss"] > 0.5, true)
  assert_eq(network_responses[5]["expected_data_loss"] > 0.5, true)
}